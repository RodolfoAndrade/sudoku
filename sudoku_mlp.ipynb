{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = pd.read_csv(\"sudoku_dataset/sudoku.csv\")\n",
    "print(dt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(string):\n",
    "    array = list(string)\n",
    "    reshaped = np.reshape(array, (9,9))\n",
    "    return pd.DataFrame(reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "puz = create_dataframe(dt[\"quizzes\"][0])\n",
    "sol = create_dataframe(dt[\"solutions\"][0])\n",
    "print(\"quiz\")\n",
    "display(puz)\n",
    "print(\"solution\")\n",
    "display(sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "def preprocessing_data(data):\n",
    "    array = [list(map(int,list(i))) for i in data]\n",
    "    return to_categorical(array, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch = 32\n",
    "train_size = 200000\n",
    "test_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0      1      2 ..., 199997 199998 199999]\n",
      "[200000 200001 200002 ..., 209997 209998 209999]\n"
     ]
    }
   ],
   "source": [
    "x_train = np.array(range(train_size))\n",
    "y_train = np.array(range(train_size))\n",
    "\n",
    "x_test = np.array(range(train_size, train_size+test_size))\n",
    "y_test = np.array(range(train_size, train_size+test_size))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = preprocessing_data(dt[\"quizzes\"][:train_size])\n",
    "# y_train = preprocessing_data(dt[\"solutions\"][:train_size])\n",
    "\n",
    "# x_test = preprocessing_data(dt[\"quizzes\"][train_size:train_size+test_size])\n",
    "# y_test = preprocessing_data(dt[\"solutions\"][train_size:train_size+test_size])\n",
    "\n",
    "# print(y_train.shape)\n",
    "# print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import Sequence\n",
    "\n",
    "class SudokuSequence(Sequence):\n",
    "\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        return preprocessing_data(dt[\"quizzes\"][batch_x]), preprocessing_data(dt[\"solutions\"][batch_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_19 (Flatten)         (None, 810)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 64)                51904     \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 810)               52650     \n",
      "_________________________________________________________________\n",
      "reshape_17 (Reshape)         (None, 81, 10)            0         \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 81, 10)            0         \n",
      "=================================================================\n",
      "Total params: 104,554\n",
      "Trainable params: 104,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 2.3022 - acc: 0.1070 - val_loss: 2.2965 - val_acc: 0.1103\n",
      "Epoch 2/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 2.2904 - acc: 0.1113 - val_loss: 2.2831 - val_acc: 0.1117\n",
      "Epoch 3/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 2.2728 - acc: 0.1118 - val_loss: 2.2612 - val_acc: 0.1120\n",
      "Epoch 4/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 2.2480 - acc: 0.1126 - val_loss: 2.2351 - val_acc: 0.1132\n",
      "Epoch 5/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 2.2247 - acc: 0.1145 - val_loss: 2.2163 - val_acc: 0.1157\n",
      "Epoch 6/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 2.2114 - acc: 0.1166 - val_loss: 2.2077 - val_acc: 0.1174\n",
      "Epoch 7/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 2.2055 - acc: 0.1182 - val_loss: 2.2036 - val_acc: 0.1188\n",
      "Epoch 8/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 2.2024 - acc: 0.1196 - val_loss: 2.2011 - val_acc: 0.1204\n",
      "Epoch 9/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 2.2003 - acc: 0.1209 - val_loss: 2.1994 - val_acc: 0.1217\n",
      "Epoch 10/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 2.1988 - acc: 0.1224 - val_loss: 2.1981 - val_acc: 0.1230\n",
      "Epoch 11/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 2.1976 - acc: 0.1238 - val_loss: 2.1970 - val_acc: 0.1245\n",
      "Epoch 12/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 2.1966 - acc: 0.1252 - val_loss: 2.1960 - val_acc: 0.1262\n",
      "Epoch 13/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 2.1957 - acc: 0.1267 - val_loss: 2.1952 - val_acc: 0.1277\n",
      "Epoch 14/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 2.1948 - acc: 0.1282 - val_loss: 2.1943 - val_acc: 0.1290\n",
      "Epoch 15/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 2.1940 - acc: 0.1296 - val_loss: 2.1935 - val_acc: 0.1307\n",
      "Epoch 16/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 2.1932 - acc: 0.1312 - val_loss: 2.1928 - val_acc: 0.1321\n",
      "Epoch 17/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 2.1924 - acc: 0.1329 - val_loss: 2.1920 - val_acc: 0.1338\n",
      "Epoch 18/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 2.1917 - acc: 0.1344 - val_loss: 2.1913 - val_acc: 0.1353\n",
      "Epoch 19/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 2.1909 - acc: 0.1360 - val_loss: 2.1905 - val_acc: 0.1371\n",
      "Epoch 20/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 2.1902 - acc: 0.1378 - val_loss: 2.1898 - val_acc: 0.1385\n",
      "Epoch 21/1000\n",
      "1562/1562 [==============================] - 37s 23ms/step - loss: 2.1894 - acc: 0.1394 - val_loss: 2.1890 - val_acc: 0.1404\n",
      "Epoch 22/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 2.1886 - acc: 0.1411 - val_loss: 2.1882 - val_acc: 0.1419\n",
      "Epoch 23/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 2.1878 - acc: 0.1428 - val_loss: 2.1874 - val_acc: 0.1440\n",
      "Epoch 24/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 2.1870 - acc: 0.1445 - val_loss: 2.1866 - val_acc: 0.1455\n",
      "Epoch 25/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 2.1862 - acc: 0.1464 - val_loss: 2.1858 - val_acc: 0.1474\n",
      "Epoch 26/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.1854 - acc: 0.1482 - val_loss: 2.1849 - val_acc: 0.1494\n",
      "Epoch 27/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.1846 - acc: 0.1500 - val_loss: 2.1841 - val_acc: 0.1511\n",
      "Epoch 28/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.1837 - acc: 0.1518 - val_loss: 2.1832 - val_acc: 0.1530\n",
      "Epoch 29/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.1828 - acc: 0.1537 - val_loss: 2.1824 - val_acc: 0.1551\n",
      "Epoch 30/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.1819 - acc: 0.1557 - val_loss: 2.1815 - val_acc: 0.1568\n",
      "Epoch 31/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.1810 - acc: 0.1576 - val_loss: 2.1806 - val_acc: 0.1585\n",
      "Epoch 32/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.1801 - acc: 0.1595 - val_loss: 2.1797 - val_acc: 0.1605\n",
      "Epoch 33/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.1792 - acc: 0.1615 - val_loss: 2.1787 - val_acc: 0.1624\n",
      "Epoch 34/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.1782 - acc: 0.1635 - val_loss: 2.1778 - val_acc: 0.1646\n",
      "Epoch 35/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.1773 - acc: 0.1655 - val_loss: 2.1768 - val_acc: 0.1664\n",
      "Epoch 36/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.1763 - acc: 0.1676 - val_loss: 2.1758 - val_acc: 0.1684\n",
      "Epoch 37/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.1753 - acc: 0.1695 - val_loss: 2.1748 - val_acc: 0.1705\n",
      "Epoch 38/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.1742 - acc: 0.1716 - val_loss: 2.1738 - val_acc: 0.1726\n",
      "Epoch 39/1000\n",
      "1562/1562 [==============================] - 40s 25ms/step - loss: 2.1732 - acc: 0.1737 - val_loss: 2.1728 - val_acc: 0.1747\n",
      "Epoch 40/1000\n",
      "1562/1562 [==============================] - 39s 25ms/step - loss: 2.1721 - acc: 0.1757 - val_loss: 2.1717 - val_acc: 0.1766\n",
      "Epoch 41/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 2.1711 - acc: 0.1778 - val_loss: 2.1707 - val_acc: 0.1790\n",
      "Epoch 42/1000\n",
      "1562/1562 [==============================] - 37s 23ms/step - loss: 2.1700 - acc: 0.1799 - val_loss: 2.1696 - val_acc: 0.1810\n",
      "Epoch 43/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.1689 - acc: 0.1821 - val_loss: 2.1685 - val_acc: 0.1829\n",
      "Epoch 44/1000\n",
      "1562/1562 [==============================] - 37s 23ms/step - loss: 2.1677 - acc: 0.1842 - val_loss: 2.1673 - val_acc: 0.1853\n",
      "Epoch 45/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 2.1666 - acc: 0.1863 - val_loss: 2.1662 - val_acc: 0.1872\n",
      "Epoch 46/1000\n",
      "1562/1562 [==============================] - 39s 25ms/step - loss: 2.1654 - acc: 0.1886 - val_loss: 2.1651 - val_acc: 0.1896\n",
      "Epoch 47/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.1642 - acc: 0.1908 - val_loss: 2.1639 - val_acc: 0.1916\n",
      "Epoch 48/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 2.1631 - acc: 0.1929 - val_loss: 2.1627 - val_acc: 0.1941\n",
      "Epoch 49/1000\n",
      "1562/1562 [==============================] - 38s 25ms/step - loss: 2.1618 - acc: 0.1952 - val_loss: 2.1615 - val_acc: 0.1959\n",
      "Epoch 50/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 2.1606 - acc: 0.1973 - val_loss: 2.1602 - val_acc: 0.1984\n",
      "Epoch 51/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.1593 - acc: 0.1995 - val_loss: 2.1590 - val_acc: 0.2008\n",
      "Epoch 52/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.1581 - acc: 0.2016 - val_loss: 2.1577 - val_acc: 0.2030\n",
      "Epoch 53/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.1568 - acc: 0.2039 - val_loss: 2.1564 - val_acc: 0.2047\n",
      "Epoch 54/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.1554 - acc: 0.2061 - val_loss: 2.1552 - val_acc: 0.2070\n",
      "Epoch 55/1000\n",
      "1562/1562 [==============================] - 39s 25ms/step - loss: 2.1541 - acc: 0.2082 - val_loss: 2.1538 - val_acc: 0.2089\n",
      "Epoch 56/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 2.1528 - acc: 0.2103 - val_loss: 2.1525 - val_acc: 0.2112\n",
      "Epoch 57/1000\n",
      "1562/1562 [==============================] - 39s 25ms/step - loss: 2.1514 - acc: 0.2125 - val_loss: 2.1511 - val_acc: 0.2136\n",
      "Epoch 58/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.1500 - acc: 0.2146 - val_loss: 2.1497 - val_acc: 0.2156\n",
      "Epoch 59/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.1486 - acc: 0.2168 - val_loss: 2.1483 - val_acc: 0.2176\n",
      "Epoch 60/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.1472 - acc: 0.2189 - val_loss: 2.1468 - val_acc: 0.2196\n",
      "Epoch 61/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.1457 - acc: 0.2209 - val_loss: 2.1454 - val_acc: 0.2218\n",
      "Epoch 62/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.1443 - acc: 0.2230 - val_loss: 2.1440 - val_acc: 0.2238\n",
      "Epoch 63/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.1428 - acc: 0.2251 - val_loss: 2.1425 - val_acc: 0.2258\n",
      "Epoch 64/1000\n",
      "1562/1562 [==============================] - 37s 23ms/step - loss: 2.1412 - acc: 0.2271 - val_loss: 2.1409 - val_acc: 0.2279\n",
      "Epoch 65/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.1397 - acc: 0.2291 - val_loss: 2.1394 - val_acc: 0.2299\n",
      "Epoch 66/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.1382 - acc: 0.2311 - val_loss: 2.1378 - val_acc: 0.2317\n",
      "Epoch 67/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.1366 - acc: 0.2331 - val_loss: 2.1363 - val_acc: 0.2336\n",
      "Epoch 68/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.1350 - acc: 0.2351 - val_loss: 2.1347 - val_acc: 0.2359\n",
      "Epoch 69/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.1334 - acc: 0.2369 - val_loss: 2.1331 - val_acc: 0.2376\n",
      "Epoch 70/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.1317 - acc: 0.2388 - val_loss: 2.1315 - val_acc: 0.2396\n",
      "Epoch 71/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.1301 - acc: 0.2407 - val_loss: 2.1298 - val_acc: 0.2413\n",
      "Epoch 72/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.1284 - acc: 0.2426 - val_loss: 2.1282 - val_acc: 0.2429\n",
      "Epoch 73/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.1267 - acc: 0.2443 - val_loss: 2.1265 - val_acc: 0.2448\n",
      "Epoch 74/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.1250 - acc: 0.2462 - val_loss: 2.1247 - val_acc: 0.2471\n",
      "Epoch 75/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.1233 - acc: 0.2478 - val_loss: 2.1230 - val_acc: 0.2484\n",
      "Epoch 76/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.1215 - acc: 0.2496 - val_loss: 2.1213 - val_acc: 0.2503\n",
      "Epoch 77/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.1197 - acc: 0.2512 - val_loss: 2.1194 - val_acc: 0.2521\n",
      "Epoch 78/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.1179 - acc: 0.2528 - val_loss: 2.1177 - val_acc: 0.2537\n",
      "Epoch 79/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 2.1160 - acc: 0.2545 - val_loss: 2.1158 - val_acc: 0.2549\n",
      "Epoch 80/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 2.1142 - acc: 0.2562 - val_loss: 2.1139 - val_acc: 0.2564\n",
      "Epoch 81/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 2.1123 - acc: 0.2577 - val_loss: 2.1121 - val_acc: 0.2581\n",
      "Epoch 82/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 2.1104 - acc: 0.2593 - val_loss: 2.1102 - val_acc: 0.2598\n",
      "Epoch 83/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 2.1085 - acc: 0.2607 - val_loss: 2.1083 - val_acc: 0.2613\n",
      "Epoch 84/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 2.1065 - acc: 0.2623 - val_loss: 2.1063 - val_acc: 0.2627\n",
      "Epoch 85/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 2.1046 - acc: 0.2637 - val_loss: 2.1044 - val_acc: 0.2639\n",
      "Epoch 86/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 2.1025 - acc: 0.2652 - val_loss: 2.1024 - val_acc: 0.2655\n",
      "Epoch 87/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 2.1005 - acc: 0.2665 - val_loss: 2.1004 - val_acc: 0.2670\n",
      "Epoch 88/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 2.0985 - acc: 0.2680 - val_loss: 2.0984 - val_acc: 0.2683\n",
      "Epoch 89/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 2.0965 - acc: 0.2693 - val_loss: 2.0964 - val_acc: 0.2696\n",
      "Epoch 90/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 2.0944 - acc: 0.2708 - val_loss: 2.0943 - val_acc: 0.2708\n",
      "Epoch 91/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 2.0923 - acc: 0.2721 - val_loss: 2.0922 - val_acc: 0.2722\n",
      "Epoch 92/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 2.0902 - acc: 0.2734 - val_loss: 2.0901 - val_acc: 0.2734\n",
      "Epoch 93/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 2.0881 - acc: 0.2747 - val_loss: 2.0880 - val_acc: 0.2749\n",
      "Epoch 94/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 2.0860 - acc: 0.2760 - val_loss: 2.0859 - val_acc: 0.2760\n",
      "Epoch 95/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 2.0838 - acc: 0.2772 - val_loss: 2.0838 - val_acc: 0.2770\n",
      "Epoch 96/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 2.0816 - acc: 0.2785 - val_loss: 2.0816 - val_acc: 0.2785\n",
      "Epoch 97/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 2.0794 - acc: 0.2797 - val_loss: 2.0794 - val_acc: 0.2795\n",
      "Epoch 98/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 2.0772 - acc: 0.2808 - val_loss: 2.0772 - val_acc: 0.2806\n",
      "Epoch 99/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 2.0750 - acc: 0.2820 - val_loss: 2.0751 - val_acc: 0.2818\n",
      "Epoch 100/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 2.0728 - acc: 0.2832 - val_loss: 2.0729 - val_acc: 0.2830\n",
      "Epoch 101/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 2.0705 - acc: 0.2843 - val_loss: 2.0706 - val_acc: 0.2841\n",
      "Epoch 102/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 2.0682 - acc: 0.2853 - val_loss: 2.0683 - val_acc: 0.2854\n",
      "Epoch 103/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 2.0660 - acc: 0.2864 - val_loss: 2.0661 - val_acc: 0.2863\n",
      "Epoch 104/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 2.0637 - acc: 0.2875 - val_loss: 2.0638 - val_acc: 0.2874\n",
      "Epoch 105/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 2.0614 - acc: 0.2885 - val_loss: 2.0615 - val_acc: 0.2883\n",
      "Epoch 106/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 2.0591 - acc: 0.2895 - val_loss: 2.0593 - val_acc: 0.2894\n",
      "Epoch 107/1000\n",
      "1562/1562 [==============================] - 39s 25ms/step - loss: 2.0568 - acc: 0.2905 - val_loss: 2.0570 - val_acc: 0.2905\n",
      "Epoch 108/1000\n",
      "1562/1562 [==============================] - 40s 25ms/step - loss: 2.0544 - acc: 0.2915 - val_loss: 2.0547 - val_acc: 0.2918\n",
      "Epoch 109/1000\n",
      "1562/1562 [==============================] - 40s 26ms/step - loss: 2.0521 - acc: 0.2925 - val_loss: 2.0524 - val_acc: 0.2925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/1000\n",
      "1562/1562 [==============================] - 41s 26ms/step - loss: 2.0497 - acc: 0.2934 - val_loss: 2.0499 - val_acc: 0.2934\n",
      "Epoch 111/1000\n",
      "1562/1562 [==============================] - 39s 25ms/step - loss: 2.0474 - acc: 0.2943 - val_loss: 2.0477 - val_acc: 0.2942: 0s - loss: 2.0475 - acc\n",
      "Epoch 112/1000\n",
      "1562/1562 [==============================] - 38s 25ms/step - loss: 2.0450 - acc: 0.2953 - val_loss: 2.0454 - val_acc: 0.2951\n",
      "Epoch 113/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 2.0426 - acc: 0.2962 - val_loss: 2.0431 - val_acc: 0.2960\n",
      "Epoch 114/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 2.0402 - acc: 0.2971 - val_loss: 2.0406 - val_acc: 0.2969\n",
      "Epoch 115/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.0379 - acc: 0.2980 - val_loss: 2.0383 - val_acc: 0.2975\n",
      "Epoch 116/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.0355 - acc: 0.2989 - val_loss: 2.0359 - val_acc: 0.2987\n",
      "Epoch 117/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.0330 - acc: 0.2997 - val_loss: 2.0335 - val_acc: 0.2997\n",
      "Epoch 118/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.0307 - acc: 0.3006 - val_loss: 2.0313 - val_acc: 0.3002\n",
      "Epoch 119/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.0283 - acc: 0.3014 - val_loss: 2.0288 - val_acc: 0.3009\n",
      "Epoch 120/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.0258 - acc: 0.3023 - val_loss: 2.0265 - val_acc: 0.3019\n",
      "Epoch 121/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.0235 - acc: 0.3031 - val_loss: 2.0241 - val_acc: 0.3026\n",
      "Epoch 122/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.0210 - acc: 0.3040 - val_loss: 2.0217 - val_acc: 0.3035\n",
      "Epoch 123/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.0185 - acc: 0.3048 - val_loss: 2.0192 - val_acc: 0.3042\n",
      "Epoch 124/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.0163 - acc: 0.3055 - val_loss: 2.0169 - val_acc: 0.3050\n",
      "Epoch 125/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.0138 - acc: 0.3064 - val_loss: 2.0146 - val_acc: 0.3059\n",
      "Epoch 126/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.0113 - acc: 0.3072 - val_loss: 2.0123 - val_acc: 0.3065\n",
      "Epoch 127/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.0090 - acc: 0.3080 - val_loss: 2.0099 - val_acc: 0.3071\n",
      "Epoch 128/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.0065 - acc: 0.3088 - val_loss: 2.0075 - val_acc: 0.3081\n",
      "Epoch 129/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.0042 - acc: 0.3094 - val_loss: 2.0050 - val_acc: 0.3089\n",
      "Epoch 130/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 2.0018 - acc: 0.3102 - val_loss: 2.0027 - val_acc: 0.3095\n",
      "Epoch 131/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9993 - acc: 0.3110 - val_loss: 2.0003 - val_acc: 0.3104\n",
      "Epoch 132/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9969 - acc: 0.3118 - val_loss: 1.9979 - val_acc: 0.3113\n",
      "Epoch 133/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9947 - acc: 0.3124 - val_loss: 1.9958 - val_acc: 0.3117\n",
      "Epoch 134/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9923 - acc: 0.3132 - val_loss: 1.9934 - val_acc: 0.3125\n",
      "Epoch 135/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9899 - acc: 0.3139 - val_loss: 1.9910 - val_acc: 0.3132\n",
      "Epoch 136/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9875 - acc: 0.3147 - val_loss: 1.9887 - val_acc: 0.3140\n",
      "Epoch 137/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9853 - acc: 0.3153 - val_loss: 1.9865 - val_acc: 0.3146\n",
      "Epoch 138/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9829 - acc: 0.3160 - val_loss: 1.9842 - val_acc: 0.3153\n",
      "Epoch 139/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9805 - acc: 0.3167 - val_loss: 1.9819 - val_acc: 0.3160\n",
      "Epoch 140/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9784 - acc: 0.3174 - val_loss: 1.9796 - val_acc: 0.3166\n",
      "Epoch 141/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9760 - acc: 0.3181 - val_loss: 1.9774 - val_acc: 0.3170\n",
      "Epoch 142/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9737 - acc: 0.3187 - val_loss: 1.9752 - val_acc: 0.3179\n",
      "Epoch 143/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9715 - acc: 0.3194 - val_loss: 1.9730 - val_acc: 0.3186\n",
      "Epoch 144/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9692 - acc: 0.3200 - val_loss: 1.9706 - val_acc: 0.3193\n",
      "Epoch 145/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9670 - acc: 0.3206 - val_loss: 1.9684 - val_acc: 0.3198\n",
      "Epoch 146/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9647 - acc: 0.3214 - val_loss: 1.9664 - val_acc: 0.3204\n",
      "Epoch 147/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9626 - acc: 0.3219 - val_loss: 1.9642 - val_acc: 0.3211\n",
      "Epoch 148/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9604 - acc: 0.3226 - val_loss: 1.9621 - val_acc: 0.3217\n",
      "Epoch 149/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9582 - acc: 0.3232 - val_loss: 1.9600 - val_acc: 0.3223\n",
      "Epoch 150/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9560 - acc: 0.3238 - val_loss: 1.9577 - val_acc: 0.3231\n",
      "Epoch 151/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9539 - acc: 0.3245 - val_loss: 1.9558 - val_acc: 0.3236\n",
      "Epoch 152/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9518 - acc: 0.3251 - val_loss: 1.9537 - val_acc: 0.3241\n",
      "Epoch 153/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9496 - acc: 0.3257 - val_loss: 1.9514 - val_acc: 0.3249\n",
      "Epoch 154/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9477 - acc: 0.3261 - val_loss: 1.9496 - val_acc: 0.3253\n",
      "Epoch 155/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9455 - acc: 0.3269 - val_loss: 1.9476 - val_acc: 0.3259\n",
      "Epoch 156/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9434 - acc: 0.3274 - val_loss: 1.9454 - val_acc: 0.3264\n",
      "Epoch 157/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9414 - acc: 0.3281 - val_loss: 1.9434 - val_acc: 0.3270\n",
      "Epoch 158/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9393 - acc: 0.3286 - val_loss: 1.9415 - val_acc: 0.3277\n",
      "Epoch 159/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9374 - acc: 0.3292 - val_loss: 1.9395 - val_acc: 0.3283\n",
      "Epoch 160/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9354 - acc: 0.3297 - val_loss: 1.9376 - val_acc: 0.3288\n",
      "Epoch 161/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9334 - acc: 0.3302 - val_loss: 1.9355 - val_acc: 0.3293\n",
      "Epoch 162/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9314 - acc: 0.3309 - val_loss: 1.9337 - val_acc: 0.3298\n",
      "Epoch 163/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9294 - acc: 0.3314 - val_loss: 1.9317 - val_acc: 0.3304\n",
      "Epoch 164/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9276 - acc: 0.3320 - val_loss: 1.9300 - val_acc: 0.3307\n",
      "Epoch 165/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9256 - acc: 0.3325 - val_loss: 1.9281 - val_acc: 0.3313\n",
      "Epoch 166/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9237 - acc: 0.3331 - val_loss: 1.9262 - val_acc: 0.3317\n",
      "Epoch 167/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9219 - acc: 0.3336 - val_loss: 1.9244 - val_acc: 0.3324\n",
      "Epoch 168/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9200 - acc: 0.3342 - val_loss: 1.9226 - val_acc: 0.3329\n",
      "Epoch 169/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9183 - acc: 0.3346 - val_loss: 1.9208 - val_acc: 0.3334\n",
      "Epoch 170/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9164 - acc: 0.3352 - val_loss: 1.9191 - val_acc: 0.3338\n",
      "Epoch 171/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9145 - acc: 0.3358 - val_loss: 1.9171 - val_acc: 0.3346\n",
      "Epoch 172/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9128 - acc: 0.3363 - val_loss: 1.9155 - val_acc: 0.3350\n",
      "Epoch 173/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9109 - acc: 0.3369 - val_loss: 1.9139 - val_acc: 0.3354\n",
      "Epoch 174/1000\n",
      "1562/1562 [==============================] - 39s 25ms/step - loss: 1.9093 - acc: 0.3373 - val_loss: 1.9122 - val_acc: 0.3360\n",
      "Epoch 175/1000\n",
      "1562/1562 [==============================] - 40s 25ms/step - loss: 1.9078 - acc: 0.3377 - val_loss: 1.9104 - val_acc: 0.3364\n",
      "Epoch 176/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9059 - acc: 0.3383 - val_loss: 1.9087 - val_acc: 0.3370\n",
      "Epoch 177/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9041 - acc: 0.3388 - val_loss: 1.9071 - val_acc: 0.3376\n",
      "Epoch 178/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.9024 - acc: 0.3394 - val_loss: 1.9057 - val_acc: 0.3378\n",
      "Epoch 179/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.9011 - acc: 0.3397 - val_loss: 1.9040 - val_acc: 0.3384\n",
      "Epoch 180/1000\n",
      "1562/1562 [==============================] - 40s 26ms/step - loss: 1.8992 - acc: 0.3403 - val_loss: 1.9024 - val_acc: 0.3388\n",
      "Epoch 181/1000\n",
      "1562/1562 [==============================] - 42s 27ms/step - loss: 1.8978 - acc: 0.3406 - val_loss: 1.9008 - val_acc: 0.3393\n",
      "Epoch 182/1000\n",
      "1562/1562 [==============================] - 40s 26ms/step - loss: 1.8960 - acc: 0.3412 - val_loss: 1.8991 - val_acc: 0.3399\n",
      "Epoch 183/1000\n",
      "1562/1562 [==============================] - 39s 25ms/step - loss: 1.8946 - acc: 0.3417 - val_loss: 1.8976 - val_acc: 0.3403\n",
      "Epoch 184/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.8929 - acc: 0.3422 - val_loss: 1.8960 - val_acc: 0.3407\n",
      "Epoch 185/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.8914 - acc: 0.3426 - val_loss: 1.8947 - val_acc: 0.3412\n",
      "Epoch 186/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.8899 - acc: 0.3431 - val_loss: 1.8932 - val_acc: 0.3417\n",
      "Epoch 187/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.8885 - acc: 0.3435 - val_loss: 1.8916 - val_acc: 0.3422\n",
      "Epoch 188/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.8869 - acc: 0.3441 - val_loss: 1.8903 - val_acc: 0.3425\n",
      "Epoch 189/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.8854 - acc: 0.3444 - val_loss: 1.8887 - val_acc: 0.3432\n",
      "Epoch 190/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.8840 - acc: 0.3449 - val_loss: 1.8874 - val_acc: 0.3433\n",
      "Epoch 191/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.8826 - acc: 0.3454 - val_loss: 1.8861 - val_acc: 0.3439\n",
      "Epoch 192/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.8813 - acc: 0.3457 - val_loss: 1.8848 - val_acc: 0.3442\n",
      "Epoch 193/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.8796 - acc: 0.3463 - val_loss: 1.8832 - val_acc: 0.3446\n",
      "Epoch 194/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.8784 - acc: 0.3467 - val_loss: 1.8820 - val_acc: 0.3452\n",
      "Epoch 195/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.8771 - acc: 0.3470 - val_loss: 1.8806 - val_acc: 0.3455\n",
      "Epoch 196/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.8757 - acc: 0.3476 - val_loss: 1.8792 - val_acc: 0.3460\n",
      "Epoch 197/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.8743 - acc: 0.3479 - val_loss: 1.8781 - val_acc: 0.3463\n",
      "Epoch 198/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.8729 - acc: 0.3484 - val_loss: 1.8769 - val_acc: 0.3467\n",
      "Epoch 199/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.8717 - acc: 0.3488 - val_loss: 1.8756 - val_acc: 0.3471\n",
      "Epoch 200/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.8706 - acc: 0.3491 - val_loss: 1.8743 - val_acc: 0.3475\n",
      "Epoch 201/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.8691 - acc: 0.3495 - val_loss: 1.8730 - val_acc: 0.3477\n",
      "Epoch 202/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.8681 - acc: 0.3499 - val_loss: 1.8717 - val_acc: 0.3485\n",
      "Epoch 203/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.8665 - acc: 0.3504 - val_loss: 1.8705 - val_acc: 0.3489\n",
      "Epoch 204/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.8655 - acc: 0.3507 - val_loss: 1.8695 - val_acc: 0.3493\n",
      "Epoch 205/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.8644 - acc: 0.3511 - val_loss: 1.8684 - val_acc: 0.3494\n",
      "Epoch 206/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.8628 - acc: 0.3516 - val_loss: 1.8672 - val_acc: 0.3499\n",
      "Epoch 207/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.8620 - acc: 0.3518 - val_loss: 1.8660 - val_acc: 0.3503\n",
      "Epoch 208/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.8607 - acc: 0.3523 - val_loss: 1.8648 - val_acc: 0.3508\n",
      "Epoch 209/1000\n",
      "1562/1562 [==============================] - 39s 25ms/step - loss: 1.8596 - acc: 0.3526 - val_loss: 1.8638 - val_acc: 0.3510\n",
      "Epoch 210/1000\n",
      "1562/1562 [==============================] - 39s 25ms/step - loss: 1.8584 - acc: 0.3531 - val_loss: 1.8626 - val_acc: 0.3514\n",
      "Epoch 211/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.8572 - acc: 0.3534 - val_loss: 1.8614 - val_acc: 0.3519\n",
      "Epoch 212/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.8563 - acc: 0.3537 - val_loss: 1.8606 - val_acc: 0.3519\n",
      "Epoch 213/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.8550 - acc: 0.3541 - val_loss: 1.8595 - val_acc: 0.3523\n",
      "Epoch 214/1000\n",
      "1562/1562 [==============================] - 39s 25ms/step - loss: 1.8541 - acc: 0.3545 - val_loss: 1.8581 - val_acc: 0.3530\n",
      "Epoch 215/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.8529 - acc: 0.3548 - val_loss: 1.8572 - val_acc: 0.3533\n",
      "Epoch 216/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.8520 - acc: 0.3552 - val_loss: 1.8563 - val_acc: 0.3535\n",
      "Epoch 217/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.8506 - acc: 0.3557 - val_loss: 1.8553 - val_acc: 0.3540\n",
      "Epoch 218/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.8498 - acc: 0.3559 - val_loss: 1.8543 - val_acc: 0.3543\n",
      "Epoch 219/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.8487 - acc: 0.3562 - val_loss: 1.8533 - val_acc: 0.3547\n",
      "Epoch 220/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.8477 - acc: 0.3567 - val_loss: 1.8524 - val_acc: 0.3548\n",
      "Epoch 221/1000\n",
      "1562/1562 [==============================] - 39s 25ms/step - loss: 1.8469 - acc: 0.3568 - val_loss: 1.8514 - val_acc: 0.3552\n",
      "Epoch 222/1000\n",
      "1562/1562 [==============================] - 39s 25ms/step - loss: 1.8456 - acc: 0.3573 - val_loss: 1.8505 - val_acc: 0.3555\n",
      "Epoch 223/1000\n",
      "1562/1562 [==============================] - 38s 25ms/step - loss: 1.8448 - acc: 0.3576 - val_loss: 1.8493 - val_acc: 0.3560\n",
      "Epoch 224/1000\n",
      "1562/1562 [==============================] - 39s 25ms/step - loss: 1.8439 - acc: 0.3580 - val_loss: 1.8484 - val_acc: 0.3563\n",
      "Epoch 225/1000\n",
      "1562/1562 [==============================] - 39s 25ms/step - loss: 1.8426 - acc: 0.3583 - val_loss: 1.8476 - val_acc: 0.3565\n",
      "Epoch 226/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.8420 - acc: 0.3585 - val_loss: 1.8467 - val_acc: 0.3569\n",
      "Epoch 227/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.8408 - acc: 0.3590 - val_loss: 1.8457 - val_acc: 0.3572\n",
      "Epoch 228/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.8402 - acc: 0.3591 - val_loss: 1.8450 - val_acc: 0.3575\n",
      "Epoch 229/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.8390 - acc: 0.3596 - val_loss: 1.8438 - val_acc: 0.3578\n",
      "Epoch 230/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.8384 - acc: 0.3599 - val_loss: 1.8433 - val_acc: 0.3581\n",
      "Epoch 231/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.8372 - acc: 0.3602 - val_loss: 1.8423 - val_acc: 0.3584\n",
      "Epoch 232/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.8365 - acc: 0.3605 - val_loss: 1.8413 - val_acc: 0.3587\n",
      "Epoch 233/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.8356 - acc: 0.3608 - val_loss: 1.8405 - val_acc: 0.3590\n",
      "Epoch 234/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.8347 - acc: 0.3612 - val_loss: 1.8398 - val_acc: 0.3591\n",
      "Epoch 235/1000\n",
      "1562/1562 [==============================] - 37s 23ms/step - loss: 1.8338 - acc: 0.3614 - val_loss: 1.8389 - val_acc: 0.3595\n",
      "Epoch 236/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.8330 - acc: 0.3617 - val_loss: 1.8380 - val_acc: 0.3598\n",
      "Epoch 237/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.8321 - acc: 0.3621 - val_loss: 1.8373 - val_acc: 0.3600\n",
      "Epoch 238/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.8315 - acc: 0.3623 - val_loss: 1.8364 - val_acc: 0.3604\n",
      "Epoch 239/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.8303 - acc: 0.3627 - val_loss: 1.8357 - val_acc: 0.3604\n",
      "Epoch 240/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.8297 - acc: 0.3630 - val_loss: 1.8348 - val_acc: 0.3609\n",
      "Epoch 241/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.8288 - acc: 0.3632 - val_loss: 1.8341 - val_acc: 0.3613\n",
      "Epoch 242/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.8281 - acc: 0.3635 - val_loss: 1.8331 - val_acc: 0.3617\n",
      "Epoch 243/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.8275 - acc: 0.3637 - val_loss: 1.8327 - val_acc: 0.3616\n",
      "Epoch 244/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.8263 - acc: 0.3641 - val_loss: 1.8319 - val_acc: 0.3619\n",
      "Epoch 245/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.8257 - acc: 0.3644 - val_loss: 1.8311 - val_acc: 0.3622\n",
      "Epoch 246/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.8250 - acc: 0.3646 - val_loss: 1.8304 - val_acc: 0.3626\n",
      "Epoch 247/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.8242 - acc: 0.3649 - val_loss: 1.8296 - val_acc: 0.3628\n",
      "Epoch 248/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.8237 - acc: 0.3652 - val_loss: 1.8290 - val_acc: 0.3631\n",
      "Epoch 249/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.8226 - acc: 0.3655 - val_loss: 1.8282 - val_acc: 0.3634\n",
      "Epoch 250/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.8221 - acc: 0.3656 - val_loss: 1.8274 - val_acc: 0.3637 -\n",
      "Epoch 251/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.8213 - acc: 0.3661 - val_loss: 1.8268 - val_acc: 0.3637\n",
      "Epoch 252/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.8204 - acc: 0.3662 - val_loss: 1.8261 - val_acc: 0.3642\n",
      "Epoch 253/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.8200 - acc: 0.3664 - val_loss: 1.8254 - val_acc: 0.3644\n",
      "Epoch 254/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.8191 - acc: 0.3668 - val_loss: 1.8247 - val_acc: 0.3645\n",
      "Epoch 255/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.8184 - acc: 0.3670 - val_loss: 1.8241 - val_acc: 0.3648\n",
      "Epoch 256/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.8179 - acc: 0.3672 - val_loss: 1.8235 - val_acc: 0.3650\n",
      "Epoch 257/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.8170 - acc: 0.3675 - val_loss: 1.8230 - val_acc: 0.3653\n",
      "Epoch 258/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.8164 - acc: 0.3678 - val_loss: 1.8222 - val_acc: 0.3655\n",
      "Epoch 259/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.8157 - acc: 0.3680 - val_loss: 1.8215 - val_acc: 0.3658\n",
      "Epoch 260/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.8152 - acc: 0.3682 - val_loss: 1.8209 - val_acc: 0.3660\n",
      "Epoch 261/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.8144 - acc: 0.3686 - val_loss: 1.8200 - val_acc: 0.3663\n",
      "Epoch 262/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.8138 - acc: 0.3686 - val_loss: 1.8196 - val_acc: 0.3664\n",
      "Epoch 263/1000\n",
      "1562/1562 [==============================] - 40s 26ms/step - loss: 1.8131 - acc: 0.3690 - val_loss: 1.8188 - val_acc: 0.3668\n",
      "Epoch 264/1000\n",
      "1562/1562 [==============================] - 40s 26ms/step - loss: 1.8125 - acc: 0.3692 - val_loss: 1.8183 - val_acc: 0.3670\n",
      "Epoch 265/1000\n",
      "1562/1562 [==============================] - 39s 25ms/step - loss: 1.8120 - acc: 0.3694 - val_loss: 1.8177 - val_acc: 0.3671\n",
      "Epoch 266/1000\n",
      "1562/1562 [==============================] - 40s 26ms/step - loss: 1.8112 - acc: 0.3697 - val_loss: 1.8171 - val_acc: 0.3675\n",
      "Epoch 267/1000\n",
      "1562/1562 [==============================] - 40s 26ms/step - loss: 1.8108 - acc: 0.3698 - val_loss: 1.8167 - val_acc: 0.3675\n",
      "Epoch 268/1000\n",
      "1562/1562 [==============================] - 40s 26ms/step - loss: 1.8101 - acc: 0.3701 - val_loss: 1.8158 - val_acc: 0.3680\n",
      "Epoch 269/1000\n",
      "1562/1562 [==============================] - 41s 26ms/step - loss: 1.8094 - acc: 0.3704 - val_loss: 1.8153 - val_acc: 0.3679\n",
      "Epoch 270/1000\n",
      "1562/1562 [==============================] - 40s 25ms/step - loss: 1.8087 - acc: 0.3706 - val_loss: 1.8148 - val_acc: 0.3683\n",
      "Epoch 271/1000\n",
      "1562/1562 [==============================] - 39s 25ms/step - loss: 1.8083 - acc: 0.3708 - val_loss: 1.8143 - val_acc: 0.3684\n",
      "Epoch 272/1000\n",
      "1562/1562 [==============================] - 38s 25ms/step - loss: 1.8076 - acc: 0.3709 - val_loss: 1.8135 - val_acc: 0.3688\n",
      "Epoch 273/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.8071 - acc: 0.3711 - val_loss: 1.8131 - val_acc: 0.3690\n",
      "Epoch 274/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.8064 - acc: 0.3716 - val_loss: 1.8124 - val_acc: 0.3692\n",
      "Epoch 275/1000\n",
      "1562/1562 [==============================] - 38s 25ms/step - loss: 1.8060 - acc: 0.3716 - val_loss: 1.8118 - val_acc: 0.3695\n",
      "Epoch 276/1000\n",
      "1562/1562 [==============================] - 38s 25ms/step - loss: 1.8052 - acc: 0.3720 - val_loss: 1.8115 - val_acc: 0.3694\n",
      "Epoch 277/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.8049 - acc: 0.3720 - val_loss: 1.8110 - val_acc: 0.3697\n",
      "Epoch 278/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.8042 - acc: 0.3723 - val_loss: 1.8104 - val_acc: 0.3700\n",
      "Epoch 279/1000\n",
      "1562/1562 [==============================] - 38s 25ms/step - loss: 1.8035 - acc: 0.3726 - val_loss: 1.8098 - val_acc: 0.3702\n",
      "Epoch 280/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.8030 - acc: 0.3727 - val_loss: 1.8092 - val_acc: 0.3705\n",
      "Epoch 281/1000\n",
      "1562/1562 [==============================] - 38s 25ms/step - loss: 1.8025 - acc: 0.3730 - val_loss: 1.8086 - val_acc: 0.3708\n",
      "Epoch 282/1000\n",
      "1562/1562 [==============================] - 38s 25ms/step - loss: 1.8020 - acc: 0.3732 - val_loss: 1.8081 - val_acc: 0.3709\n",
      "Epoch 283/1000\n",
      "1562/1562 [==============================] - 39s 25ms/step - loss: 1.8015 - acc: 0.3733 - val_loss: 1.8078 - val_acc: 0.3710\n",
      "Epoch 284/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.8009 - acc: 0.3736 - val_loss: 1.8074 - val_acc: 0.3713\n",
      "Epoch 285/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.8004 - acc: 0.3738 - val_loss: 1.8068 - val_acc: 0.3714\n",
      "Epoch 286/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7999 - acc: 0.3739 - val_loss: 1.8063 - val_acc: 0.3715\n",
      "Epoch 287/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7994 - acc: 0.3742 - val_loss: 1.8056 - val_acc: 0.3719\n",
      "Epoch 288/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7986 - acc: 0.3744 - val_loss: 1.8053 - val_acc: 0.3719\n",
      "Epoch 289/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.7985 - acc: 0.3745 - val_loss: 1.8048 - val_acc: 0.3722\n",
      "Epoch 290/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7977 - acc: 0.3747 - val_loss: 1.8041 - val_acc: 0.3724\n",
      "Epoch 291/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7974 - acc: 0.3749 - val_loss: 1.8039 - val_acc: 0.3727\n",
      "Epoch 292/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7966 - acc: 0.3753 - val_loss: 1.8035 - val_acc: 0.3728\n",
      "Epoch 293/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7964 - acc: 0.3753 - val_loss: 1.8026 - val_acc: 0.3731\n",
      "Epoch 294/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7958 - acc: 0.3756 - val_loss: 1.8022 - val_acc: 0.3730\n",
      "Epoch 295/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7954 - acc: 0.3757 - val_loss: 1.8018 - val_acc: 0.3733\n",
      "Epoch 296/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7948 - acc: 0.3759 - val_loss: 1.8014 - val_acc: 0.3735\n",
      "Epoch 297/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7945 - acc: 0.3760 - val_loss: 1.8010 - val_acc: 0.3736\n",
      "Epoch 298/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7938 - acc: 0.3763 - val_loss: 1.8006 - val_acc: 0.3740\n",
      "Epoch 299/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7936 - acc: 0.3763 - val_loss: 1.8001 - val_acc: 0.3741\n",
      "Epoch 300/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7928 - acc: 0.3767 - val_loss: 1.7997 - val_acc: 0.3742\n",
      "Epoch 301/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7927 - acc: 0.3768 - val_loss: 1.7994 - val_acc: 0.3743\n",
      "Epoch 302/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7922 - acc: 0.3770 - val_loss: 1.7986 - val_acc: 0.3747\n",
      "Epoch 303/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7916 - acc: 0.3771 - val_loss: 1.7981 - val_acc: 0.3748\n",
      "Epoch 304/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7912 - acc: 0.3774 - val_loss: 1.7980 - val_acc: 0.3749\n",
      "Epoch 305/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7908 - acc: 0.3775 - val_loss: 1.7975 - val_acc: 0.3751 - E\n",
      "Epoch 306/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7902 - acc: 0.3778 - val_loss: 1.7970 - val_acc: 0.3754\n",
      "Epoch 307/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7898 - acc: 0.3779 - val_loss: 1.7968 - val_acc: 0.3755s: 1.789\n",
      "Epoch 308/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7894 - acc: 0.3781 - val_loss: 1.7961 - val_acc: 0.3755\n",
      "Epoch 309/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7891 - acc: 0.3782 - val_loss: 1.7960 - val_acc: 0.3758\n",
      "Epoch 310/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7885 - acc: 0.3784 - val_loss: 1.7955 - val_acc: 0.3758\n",
      "Epoch 311/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7881 - acc: 0.3786 - val_loss: 1.7950 - val_acc: 0.3762\n",
      "Epoch 312/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7878 - acc: 0.3787 - val_loss: 1.7945 - val_acc: 0.3765\n",
      "Epoch 313/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7872 - acc: 0.3790 - val_loss: 1.7943 - val_acc: 0.3764\n",
      "Epoch 314/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7869 - acc: 0.3791 - val_loss: 1.7938 - val_acc: 0.3767\n",
      "Epoch 315/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7865 - acc: 0.3791 - val_loss: 1.7934 - val_acc: 0.3769\n",
      "Epoch 316/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7859 - acc: 0.3795 - val_loss: 1.7930 - val_acc: 0.3770\n",
      "Epoch 317/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7859 - acc: 0.3795 - val_loss: 1.7925 - val_acc: 0.3773\n",
      "Epoch 318/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7851 - acc: 0.3797 - val_loss: 1.7922 - val_acc: 0.3774\n",
      "Epoch 319/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7850 - acc: 0.3798 - val_loss: 1.7917 - val_acc: 0.3775\n",
      "Epoch 320/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7843 - acc: 0.3801 - val_loss: 1.7914 - val_acc: 0.3777\n",
      "Epoch 321/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7840 - acc: 0.3802 - val_loss: 1.7908 - val_acc: 0.3779\n",
      "Epoch 322/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7837 - acc: 0.3803 - val_loss: 1.7908 - val_acc: 0.3781\n",
      "Epoch 323/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7832 - acc: 0.3806 - val_loss: 1.7905 - val_acc: 0.3783.7832\n",
      "Epoch 324/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7830 - acc: 0.3805 - val_loss: 1.7899 - val_acc: 0.3782\n",
      "Epoch 325/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7824 - acc: 0.3809 - val_loss: 1.7897 - val_acc: 0.3785\n",
      "Epoch 326/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7819 - acc: 0.3811 - val_loss: 1.7892 - val_acc: 0.3787\n",
      "Epoch 327/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7819 - acc: 0.3811 - val_loss: 1.7889 - val_acc: 0.3790\n",
      "Epoch 328/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7814 - acc: 0.3813 - val_loss: 1.7885 - val_acc: 0.3790\n",
      "Epoch 329/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7810 - acc: 0.3814 - val_loss: 1.7880 - val_acc: 0.3792\n",
      "Epoch 330/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7807 - acc: 0.3815 - val_loss: 1.7878 - val_acc: 0.3794\n",
      "Epoch 331/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7803 - acc: 0.3818 - val_loss: 1.7874 - val_acc: 0.3794\n",
      "Epoch 332/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7798 - acc: 0.3818 - val_loss: 1.7870 - val_acc: 0.3797: 1s \n",
      "Epoch 333/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7792 - acc: 0.3822 - val_loss: 1.7866 - val_acc: 0.3797\n",
      "Epoch 334/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7791 - acc: 0.3821 - val_loss: 1.7864 - val_acc: 0.3799\n",
      "Epoch 335/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7789 - acc: 0.3823 - val_loss: 1.7861 - val_acc: 0.3799\n",
      "Epoch 336/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7782 - acc: 0.3825 - val_loss: 1.7859 - val_acc: 0.3800\n",
      "Epoch 337/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7780 - acc: 0.3826 - val_loss: 1.7854 - val_acc: 0.3804\n",
      "Epoch 338/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7778 - acc: 0.3828 - val_loss: 1.7850 - val_acc: 0.3805\n",
      "Epoch 339/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7772 - acc: 0.3830 - val_loss: 1.7847 - val_acc: 0.3807\n",
      "Epoch 340/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7769 - acc: 0.3831 - val_loss: 1.7841 - val_acc: 0.3808\n",
      "Epoch 341/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7768 - acc: 0.3831 - val_loss: 1.7840 - val_acc: 0.3809\n",
      "Epoch 342/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7762 - acc: 0.3834 - val_loss: 1.7834 - val_acc: 0.3811\n",
      "Epoch 343/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7759 - acc: 0.3835 - val_loss: 1.7832 - val_acc: 0.3812\n",
      "Epoch 344/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7755 - acc: 0.3837 - val_loss: 1.7829 - val_acc: 0.3813\n",
      "Epoch 345/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7754 - acc: 0.3837 - val_loss: 1.7826 - val_acc: 0.3813\n",
      "Epoch 346/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7749 - acc: 0.3839 - val_loss: 1.7824 - val_acc: 0.3816\n",
      "Epoch 347/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7747 - acc: 0.3840 - val_loss: 1.7817 - val_acc: 0.3817\n",
      "Epoch 348/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7740 - acc: 0.3843 - val_loss: 1.7816 - val_acc: 0.3818\n",
      "Epoch 349/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7738 - acc: 0.3844 - val_loss: 1.7814 - val_acc: 0.3819\n",
      "Epoch 350/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7734 - acc: 0.3845 - val_loss: 1.7811 - val_acc: 0.3820\n",
      "Epoch 351/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7734 - acc: 0.3845 - val_loss: 1.7805 - val_acc: 0.3822\n",
      "Epoch 352/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7729 - acc: 0.3847 - val_loss: 1.7805 - val_acc: 0.3822\n",
      "Epoch 353/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7726 - acc: 0.3849 - val_loss: 1.7800 - val_acc: 0.3823\n",
      "Epoch 354/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7721 - acc: 0.3851 - val_loss: 1.7796 - val_acc: 0.3826\n",
      "Epoch 355/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7721 - acc: 0.3851 - val_loss: 1.7796 - val_acc: 0.3826\n",
      "Epoch 356/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7714 - acc: 0.3853 - val_loss: 1.7793 - val_acc: 0.3827\n",
      "Epoch 357/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7714 - acc: 0.3853 - val_loss: 1.7790 - val_acc: 0.3829\n",
      "Epoch 358/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7708 - acc: 0.3856 - val_loss: 1.7786 - val_acc: 0.3831\n",
      "Epoch 359/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7706 - acc: 0.3857 - val_loss: 1.7782 - val_acc: 0.3831\n",
      "Epoch 360/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7703 - acc: 0.3857 - val_loss: 1.7778 - val_acc: 0.3834\n",
      "Epoch 361/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7699 - acc: 0.3860 - val_loss: 1.7775 - val_acc: 0.3834\n",
      "Epoch 362/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7698 - acc: 0.3860 - val_loss: 1.7774 - val_acc: 0.3836\n",
      "Epoch 363/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7692 - acc: 0.3862 - val_loss: 1.7773 - val_acc: 0.3834\n",
      "Epoch 364/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7689 - acc: 0.3864 - val_loss: 1.7768 - val_acc: 0.3835\n",
      "Epoch 365/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7690 - acc: 0.3863 - val_loss: 1.7765 - val_acc: 0.3837\n",
      "Epoch 366/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7682 - acc: 0.3867 - val_loss: 1.7761 - val_acc: 0.3840\n",
      "Epoch 367/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7681 - acc: 0.3866 - val_loss: 1.7760 - val_acc: 0.3839\n",
      "Epoch 368/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7680 - acc: 0.3868 - val_loss: 1.7757 - val_acc: 0.3841\n",
      "Epoch 369/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7672 - acc: 0.3870 - val_loss: 1.7752 - val_acc: 0.3844\n",
      "Epoch 370/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7676 - acc: 0.3869 - val_loss: 1.7752 - val_acc: 0.3845\n",
      "Epoch 371/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7668 - acc: 0.3873 - val_loss: 1.7750 - val_acc: 0.3845\n",
      "Epoch 372/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7667 - acc: 0.3872 - val_loss: 1.7742 - val_acc: 0.3849\n",
      "Epoch 373/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7662 - acc: 0.3875 - val_loss: 1.7740 - val_acc: 0.3848\n",
      "Epoch 374/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7662 - acc: 0.3875 - val_loss: 1.7738 - val_acc: 0.3848\n",
      "Epoch 375/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7659 - acc: 0.3877 - val_loss: 1.7736 - val_acc: 0.3851\n",
      "Epoch 376/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7654 - acc: 0.3878 - val_loss: 1.7734 - val_acc: 0.3852\n",
      "Epoch 377/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7651 - acc: 0.3881 - val_loss: 1.7732 - val_acc: 0.3852\n",
      "Epoch 378/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7652 - acc: 0.3880 - val_loss: 1.7727 - val_acc: 0.3856\n",
      "Epoch 379/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7646 - acc: 0.3882 - val_loss: 1.7726 - val_acc: 0.3855\n",
      "Epoch 380/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7642 - acc: 0.3883 - val_loss: 1.7725 - val_acc: 0.3855\n",
      "Epoch 381/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7641 - acc: 0.3884 - val_loss: 1.7718 - val_acc: 0.3858\n",
      "Epoch 382/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7636 - acc: 0.3886 - val_loss: 1.7715 - val_acc: 0.3860\n",
      "Epoch 383/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7635 - acc: 0.3886 - val_loss: 1.7715 - val_acc: 0.3859\n",
      "Epoch 384/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7634 - acc: 0.3888 - val_loss: 1.7713 - val_acc: 0.3861\n",
      "Epoch 385/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7632 - acc: 0.3887 - val_loss: 1.7709 - val_acc: 0.3862\n",
      "Epoch 386/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7624 - acc: 0.3891 - val_loss: 1.7708 - val_acc: 0.3862\n",
      "Epoch 387/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7625 - acc: 0.3890 - val_loss: 1.7703 - val_acc: 0.3864\n",
      "Epoch 388/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7622 - acc: 0.3893 - val_loss: 1.7704 - val_acc: 0.3865\n",
      "Epoch 389/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7620 - acc: 0.3892 - val_loss: 1.7700 - val_acc: 0.3866\n",
      "Epoch 390/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7616 - acc: 0.3895 - val_loss: 1.7697 - val_acc: 0.3869\n",
      "Epoch 391/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7614 - acc: 0.3895 - val_loss: 1.7693 - val_acc: 0.3870\n",
      "Epoch 392/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7611 - acc: 0.3897 - val_loss: 1.7694 - val_acc: 0.3868\n",
      "Epoch 393/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7608 - acc: 0.3897 - val_loss: 1.7689 - val_acc: 0.3870\n",
      "Epoch 394/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7608 - acc: 0.3898 - val_loss: 1.7686 - val_acc: 0.3872\n",
      "Epoch 395/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7602 - acc: 0.3900 - val_loss: 1.7685 - val_acc: 0.3875\n",
      "Epoch 396/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7599 - acc: 0.3901 - val_loss: 1.7682 - val_acc: 0.3875 acc: 0.39\n",
      "Epoch 397/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7600 - acc: 0.3902 - val_loss: 1.7680 - val_acc: 0.3876\n",
      "Epoch 398/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7597 - acc: 0.3903 - val_loss: 1.7676 - val_acc: 0.3877\n",
      "Epoch 399/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7593 - acc: 0.3904 - val_loss: 1.7674 - val_acc: 0.3879\n",
      "Epoch 400/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7592 - acc: 0.3905 - val_loss: 1.7671 - val_acc: 0.3881\n",
      "Epoch 401/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7588 - acc: 0.3907 - val_loss: 1.7672 - val_acc: 0.3879\n",
      "Epoch 402/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7586 - acc: 0.3908 - val_loss: 1.7670 - val_acc: 0.3880\n",
      "Epoch 403/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7585 - acc: 0.3908 - val_loss: 1.7666 - val_acc: 0.3881\n",
      "Epoch 404/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7581 - acc: 0.3909 - val_loss: 1.7666 - val_acc: 0.3883\n",
      "Epoch 405/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7580 - acc: 0.3911 - val_loss: 1.7661 - val_acc: 0.3886\n",
      "Epoch 406/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7577 - acc: 0.3910 - val_loss: 1.7660 - val_acc: 0.3884\n",
      "Epoch 407/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7577 - acc: 0.3912 - val_loss: 1.7657 - val_acc: 0.3886\n",
      "Epoch 408/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7570 - acc: 0.3914 - val_loss: 1.7654 - val_acc: 0.3886570 - acc: 0\n",
      "Epoch 409/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7571 - acc: 0.3915 - val_loss: 1.7652 - val_acc: 0.3888\n",
      "Epoch 410/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7568 - acc: 0.3913 - val_loss: 1.7650 - val_acc: 0.3887 1.75\n",
      "Epoch 411/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7564 - acc: 0.3917 - val_loss: 1.7649 - val_acc: 0.3889\n",
      "Epoch 412/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7564 - acc: 0.3916 - val_loss: 1.7645 - val_acc: 0.3888\n",
      "Epoch 413/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7563 - acc: 0.3918 - val_loss: 1.7645 - val_acc: 0.3890\n",
      "Epoch 414/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7556 - acc: 0.3920 - val_loss: 1.7644 - val_acc: 0.3890\n",
      "Epoch 415/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7558 - acc: 0.3919 - val_loss: 1.7642 - val_acc: 0.3893\n",
      "Epoch 416/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7555 - acc: 0.3921 - val_loss: 1.7639 - val_acc: 0.3893\n",
      "Epoch 417/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7553 - acc: 0.3922 - val_loss: 1.7636 - val_acc: 0.3894\n",
      "Epoch 418/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7547 - acc: 0.3923 - val_loss: 1.7634 - val_acc: 0.3896\n",
      "Epoch 419/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7549 - acc: 0.3923 - val_loss: 1.7631 - val_acc: 0.3897\n",
      "Epoch 420/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7547 - acc: 0.3924 - val_loss: 1.7630 - val_acc: 0.3896\n",
      "Epoch 421/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7544 - acc: 0.3925 - val_loss: 1.7626 - val_acc: 0.3897\n",
      "Epoch 422/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7541 - acc: 0.3927 - val_loss: 1.7625 - val_acc: 0.3899\n",
      "Epoch 423/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7541 - acc: 0.3925 - val_loss: 1.7624 - val_acc: 0.3899\n",
      "Epoch 424/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7538 - acc: 0.3928 - val_loss: 1.7622 - val_acc: 0.3899\n",
      "Epoch 425/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7536 - acc: 0.3929 - val_loss: 1.7622 - val_acc: 0.3898\n",
      "Epoch 426/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7534 - acc: 0.3929 - val_loss: 1.7616 - val_acc: 0.3903\n",
      "Epoch 427/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7532 - acc: 0.3931 - val_loss: 1.7615 - val_acc: 0.3901\n",
      "Epoch 428/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7531 - acc: 0.3931 - val_loss: 1.7615 - val_acc: 0.3903\n",
      "Epoch 429/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7525 - acc: 0.3934 - val_loss: 1.7612 - val_acc: 0.3903\n",
      "Epoch 430/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7528 - acc: 0.3933 - val_loss: 1.7609 - val_acc: 0.3904\n",
      "Epoch 431/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7524 - acc: 0.3932 - val_loss: 1.7610 - val_acc: 0.3905\n",
      "Epoch 432/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7523 - acc: 0.3935 - val_loss: 1.7607 - val_acc: 0.3906\n",
      "Epoch 433/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7520 - acc: 0.3936 - val_loss: 1.7604 - val_acc: 0.3907\n",
      "Epoch 434/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7519 - acc: 0.3936 - val_loss: 1.7606 - val_acc: 0.3905\n",
      "Epoch 435/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7515 - acc: 0.3936 - val_loss: 1.7604 - val_acc: 0.3907\n",
      "Epoch 436/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7515 - acc: 0.3939 - val_loss: 1.7601 - val_acc: 0.3908\n",
      "Epoch 437/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7514 - acc: 0.3938 - val_loss: 1.7599 - val_acc: 0.3909\n",
      "Epoch 438/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7511 - acc: 0.3940 - val_loss: 1.7596 - val_acc: 0.3911\n",
      "Epoch 439/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7508 - acc: 0.3940 - val_loss: 1.7593 - val_acc: 0.3910\n",
      "Epoch 440/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7509 - acc: 0.3940 - val_loss: 1.7592 - val_acc: 0.3911\n",
      "Epoch 441/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7507 - acc: 0.3941 - val_loss: 1.7593 - val_acc: 0.3911\n",
      "Epoch 442/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7504 - acc: 0.3944 - val_loss: 1.7592 - val_acc: 0.3912\n",
      "Epoch 443/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7502 - acc: 0.3943 - val_loss: 1.7589 - val_acc: 0.3912\n",
      "Epoch 444/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7504 - acc: 0.3943 - val_loss: 1.7587 - val_acc: 0.3912\n",
      "Epoch 445/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7494 - acc: 0.3946 - val_loss: 1.7585 - val_acc: 0.3915\n",
      "Epoch 446/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7499 - acc: 0.3945 - val_loss: 1.7585 - val_acc: 0.3914\n",
      "Epoch 447/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7494 - acc: 0.3948 - val_loss: 1.7583 - val_acc: 0.3915\n",
      "Epoch 448/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7497 - acc: 0.3945 - val_loss: 1.7580 - val_acc: 0.3918\n",
      "Epoch 449/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7490 - acc: 0.3949 - val_loss: 1.7581 - val_acc: 0.3918\n",
      "Epoch 450/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7491 - acc: 0.3948 - val_loss: 1.7580 - val_acc: 0.3916\n",
      "Epoch 451/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7491 - acc: 0.3949 - val_loss: 1.7573 - val_acc: 0.3918\n",
      "Epoch 452/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7488 - acc: 0.3950 - val_loss: 1.7572 - val_acc: 0.3919\n",
      "Epoch 453/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7485 - acc: 0.3950 - val_loss: 1.7572 - val_acc: 0.3920\n",
      "Epoch 454/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7482 - acc: 0.3952 - val_loss: 1.7572 - val_acc: 0.3919\n",
      "Epoch 455/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7485 - acc: 0.3952 - val_loss: 1.7570 - val_acc: 0.3921\n",
      "Epoch 456/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7481 - acc: 0.3952 - val_loss: 1.7570 - val_acc: 0.3921\n",
      "Epoch 457/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7480 - acc: 0.3953 - val_loss: 1.7567 - val_acc: 0.3922\n",
      "Epoch 458/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7477 - acc: 0.3954 - val_loss: 1.7566 - val_acc: 0.3924\n",
      "Epoch 459/1000\n",
      "1562/1562 [==============================] - 37s 23ms/step - loss: 1.7477 - acc: 0.3954 - val_loss: 1.7566 - val_acc: 0.3923\n",
      "Epoch 460/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7474 - acc: 0.3956 - val_loss: 1.7561 - val_acc: 0.3925\n",
      "Epoch 461/1000\n",
      "1562/1562 [==============================] - 37s 23ms/step - loss: 1.7476 - acc: 0.3954 - val_loss: 1.7559 - val_acc: 0.3924\n",
      "Epoch 462/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7471 - acc: 0.3957 - val_loss: 1.7561 - val_acc: 0.3923\n",
      "Epoch 463/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7471 - acc: 0.3957 - val_loss: 1.7560 - val_acc: 0.3925\n",
      "Epoch 464/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7471 - acc: 0.3958 - val_loss: 1.7557 - val_acc: 0.3928\n",
      "Epoch 465/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7463 - acc: 0.3959 - val_loss: 1.7558 - val_acc: 0.3927\n",
      "Epoch 466/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7471 - acc: 0.3958 - val_loss: 1.7553 - val_acc: 0.3928\n",
      "Epoch 467/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7463 - acc: 0.3959 - val_loss: 1.7556 - val_acc: 0.3926\n",
      "Epoch 468/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7464 - acc: 0.3960 - val_loss: 1.7553 - val_acc: 0.3927\n",
      "Epoch 469/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7462 - acc: 0.3961 - val_loss: 1.7552 - val_acc: 0.3927\n",
      "Epoch 470/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7459 - acc: 0.3961 - val_loss: 1.7549 - val_acc: 0.3931\n",
      "Epoch 471/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7460 - acc: 0.3962 - val_loss: 1.7550 - val_acc: 0.3928\n",
      "Epoch 472/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7457 - acc: 0.3964 - val_loss: 1.7547 - val_acc: 0.3929\n",
      "Epoch 473/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7459 - acc: 0.3963 - val_loss: 1.7544 - val_acc: 0.3932\n",
      "Epoch 474/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7453 - acc: 0.3964 - val_loss: 1.7545 - val_acc: 0.3932\n",
      "Epoch 475/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7456 - acc: 0.3964 - val_loss: 1.7543 - val_acc: 0.3931\n",
      "Epoch 476/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7453 - acc: 0.3965 - val_loss: 1.7541 - val_acc: 0.3933\n",
      "Epoch 477/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7450 - acc: 0.3966 - val_loss: 1.7539 - val_acc: 0.3935\n",
      "Epoch 478/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7450 - acc: 0.3967 - val_loss: 1.7539 - val_acc: 0.3934\n",
      "Epoch 479/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7448 - acc: 0.3967 - val_loss: 1.7536 - val_acc: 0.3934\n",
      "Epoch 480/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7447 - acc: 0.3968 - val_loss: 1.7539 - val_acc: 0.3934\n",
      "Epoch 481/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7447 - acc: 0.3967 - val_loss: 1.7538 - val_acc: 0.3935\n",
      "Epoch 482/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7444 - acc: 0.3969 - val_loss: 1.7535 - val_acc: 0.3936\n",
      "Epoch 483/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7446 - acc: 0.3968 - val_loss: 1.7535 - val_acc: 0.3937\n",
      "Epoch 484/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7441 - acc: 0.3970 - val_loss: 1.7531 - val_acc: 0.3938\n",
      "Epoch 485/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7443 - acc: 0.3970 - val_loss: 1.7532 - val_acc: 0.3938\n",
      "Epoch 486/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7437 - acc: 0.3972 - val_loss: 1.7529 - val_acc: 0.3938\n",
      "Epoch 487/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7440 - acc: 0.3972 - val_loss: 1.7528 - val_acc: 0.3940\n",
      "Epoch 488/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7437 - acc: 0.3972 - val_loss: 1.7527 - val_acc: 0.3938\n",
      "Epoch 489/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7436 - acc: 0.3973 - val_loss: 1.7526 - val_acc: 0.3940\n",
      "Epoch 490/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7436 - acc: 0.3973 - val_loss: 1.7525 - val_acc: 0.3940\n",
      "Epoch 491/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7433 - acc: 0.3974 - val_loss: 1.7523 - val_acc: 0.3941\n",
      "Epoch 492/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7434 - acc: 0.3973 - val_loss: 1.7524 - val_acc: 0.3941\n",
      "Epoch 493/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7430 - acc: 0.3976 - val_loss: 1.7525 - val_acc: 0.3940\n",
      "Epoch 494/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7431 - acc: 0.3974 - val_loss: 1.7523 - val_acc: 0.3940\n",
      "Epoch 495/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7430 - acc: 0.3976 - val_loss: 1.7522 - val_acc: 0.3942\n",
      "Epoch 496/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7429 - acc: 0.3976 - val_loss: 1.7519 - val_acc: 0.3942\n",
      "Epoch 497/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7427 - acc: 0.3978 - val_loss: 1.7517 - val_acc: 0.3944\n",
      "Epoch 498/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7425 - acc: 0.3977 - val_loss: 1.7516 - val_acc: 0.3944\n",
      "Epoch 499/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7426 - acc: 0.3978 - val_loss: 1.7516 - val_acc: 0.3943\n",
      "Epoch 500/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7423 - acc: 0.3980 - val_loss: 1.7513 - val_acc: 0.3945\n",
      "Epoch 501/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7423 - acc: 0.3978 - val_loss: 1.7513 - val_acc: 0.3947\n",
      "Epoch 502/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7422 - acc: 0.3979 - val_loss: 1.7512 - val_acc: 0.3946\n",
      "Epoch 503/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7419 - acc: 0.3982 - val_loss: 1.7511 - val_acc: 0.3946\n",
      "Epoch 504/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7420 - acc: 0.3980 - val_loss: 1.7512 - val_acc: 0.3947\n",
      "Epoch 505/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7419 - acc: 0.3982 - val_loss: 1.7507 - val_acc: 0.3948\n",
      "Epoch 506/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7416 - acc: 0.3982 - val_loss: 1.7508 - val_acc: 0.3947\n",
      "Epoch 507/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.7417 - acc: 0.3982 - val_loss: 1.7508 - val_acc: 0.3948\n",
      "Epoch 508/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7415 - acc: 0.3983 - val_loss: 1.7507 - val_acc: 0.3949\n",
      "Epoch 509/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7414 - acc: 0.3983 - val_loss: 1.7504 - val_acc: 0.3950\n",
      "Epoch 510/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7414 - acc: 0.3983 - val_loss: 1.7505 - val_acc: 0.3949\n",
      "Epoch 511/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7410 - acc: 0.3984 - val_loss: 1.7503 - val_acc: 0.3951\n",
      "Epoch 512/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.7412 - acc: 0.3985 - val_loss: 1.7501 - val_acc: 0.3950\n",
      "Epoch 513/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.7409 - acc: 0.3986 - val_loss: 1.7504 - val_acc: 0.3949\n",
      "Epoch 514/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.7411 - acc: 0.3984 - val_loss: 1.7502 - val_acc: 0.3950\n",
      "Epoch 515/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.7407 - acc: 0.3987 - val_loss: 1.7501 - val_acc: 0.3950\n",
      "Epoch 516/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7409 - acc: 0.3986 - val_loss: 1.7500 - val_acc: 0.3949\n",
      "Epoch 517/1000\n",
      "1562/1562 [==============================] - 37s 23ms/step - loss: 1.7404 - acc: 0.3987 - val_loss: 1.7497 - val_acc: 0.3952\n",
      "Epoch 518/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7405 - acc: 0.3988 - val_loss: 1.7495 - val_acc: 0.3952\n",
      "Epoch 519/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7405 - acc: 0.3988 - val_loss: 1.7495 - val_acc: 0.3952\n",
      "Epoch 520/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7403 - acc: 0.3988 - val_loss: 1.7496 - val_acc: 0.3952\n",
      "Epoch 521/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7400 - acc: 0.3989 - val_loss: 1.7497 - val_acc: 0.3952\n",
      "Epoch 522/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7403 - acc: 0.3989 - val_loss: 1.7495 - val_acc: 0.3952\n",
      "Epoch 523/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7401 - acc: 0.3990 - val_loss: 1.7492 - val_acc: 0.3952\n",
      "Epoch 524/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7398 - acc: 0.3991 - val_loss: 1.7491 - val_acc: 0.3955\n",
      "Epoch 525/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7402 - acc: 0.3989 - val_loss: 1.7493 - val_acc: 0.3952\n",
      "Epoch 526/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7397 - acc: 0.3991 - val_loss: 1.7491 - val_acc: 0.3954\n",
      "Epoch 527/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7395 - acc: 0.3993 - val_loss: 1.7488 - val_acc: 0.3955\n",
      "Epoch 528/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7394 - acc: 0.3992 - val_loss: 1.7490 - val_acc: 0.3955\n",
      "Epoch 529/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7395 - acc: 0.3993 - val_loss: 1.7490 - val_acc: 0.3954\n",
      "Epoch 530/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7393 - acc: 0.3993 - val_loss: 1.7484 - val_acc: 0.3956\n",
      "Epoch 531/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7395 - acc: 0.3992 - val_loss: 1.7483 - val_acc: 0.3956\n",
      "Epoch 532/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7389 - acc: 0.3994 - val_loss: 1.7484 - val_acc: 0.3956\n",
      "Epoch 533/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7392 - acc: 0.3994 - val_loss: 1.7485 - val_acc: 0.3956\n",
      "Epoch 534/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7391 - acc: 0.3994 - val_loss: 1.7483 - val_acc: 0.3958\n",
      "Epoch 535/1000\n",
      "1562/1562 [==============================] - 35s 23ms/step - loss: 1.7388 - acc: 0.3996 - val_loss: 1.7484 - val_acc: 0.3956\n",
      "Epoch 536/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7391 - acc: 0.3994 - val_loss: 1.7481 - val_acc: 0.3959\n",
      "Epoch 537/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7384 - acc: 0.3997 - val_loss: 1.7482 - val_acc: 0.3958\n",
      "Epoch 538/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7387 - acc: 0.3996 - val_loss: 1.7482 - val_acc: 0.3958\n",
      "Epoch 539/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7388 - acc: 0.3996 - val_loss: 1.7477 - val_acc: 0.3961\n",
      "Epoch 540/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7384 - acc: 0.3997 - val_loss: 1.7475 - val_acc: 0.3959\n",
      "Epoch 541/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.7382 - acc: 0.3998 - val_loss: 1.7479 - val_acc: 0.3961\n",
      "Epoch 542/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.7385 - acc: 0.3997 - val_loss: 1.7478 - val_acc: 0.3958\n",
      "Epoch 543/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.7382 - acc: 0.3998 - val_loss: 1.7476 - val_acc: 0.3960\n",
      "Epoch 544/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7385 - acc: 0.3998 - val_loss: 1.7477 - val_acc: 0.3960\n",
      "Epoch 545/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.7381 - acc: 0.3998 - val_loss: 1.7473 - val_acc: 0.3960\n",
      "Epoch 546/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7377 - acc: 0.4000 - val_loss: 1.7476 - val_acc: 0.3959\n",
      "Epoch 547/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7380 - acc: 0.4000 - val_loss: 1.7473 - val_acc: 0.3962\n",
      "Epoch 548/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7378 - acc: 0.4000 - val_loss: 1.7473 - val_acc: 0.3961\n",
      "Epoch 549/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.7380 - acc: 0.4001 - val_loss: 1.7471 - val_acc: 0.3964\n",
      "Epoch 550/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7374 - acc: 0.4002 - val_loss: 1.7473 - val_acc: 0.3961\n",
      "Epoch 551/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7379 - acc: 0.4000 - val_loss: 1.7470 - val_acc: 0.3962\n",
      "Epoch 552/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.7371 - acc: 0.4003 - val_loss: 1.7468 - val_acc: 0.3963\n",
      "Epoch 553/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7376 - acc: 0.4001 - val_loss: 1.7469 - val_acc: 0.3964\n",
      "Epoch 554/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.7375 - acc: 0.4002 - val_loss: 1.7468 - val_acc: 0.3964\n",
      "Epoch 555/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.7373 - acc: 0.4004 - val_loss: 1.7467 - val_acc: 0.3963\n",
      "Epoch 556/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.7371 - acc: 0.4004 - val_loss: 1.7464 - val_acc: 0.3965\n",
      "Epoch 557/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.7371 - acc: 0.4003 - val_loss: 1.7465 - val_acc: 0.3964\n",
      "Epoch 558/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.7371 - acc: 0.4005 - val_loss: 1.7463 - val_acc: 0.3966\n",
      "Epoch 559/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.7370 - acc: 0.4004 - val_loss: 1.7466 - val_acc: 0.3966\n",
      "Epoch 560/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.7367 - acc: 0.4007 - val_loss: 1.7465 - val_acc: 0.3966\n",
      "Epoch 561/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.7369 - acc: 0.4005 - val_loss: 1.7464 - val_acc: 0.3965\n",
      "Epoch 562/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.7366 - acc: 0.4006 - val_loss: 1.7464 - val_acc: 0.3965\n",
      "Epoch 563/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.7369 - acc: 0.4005 - val_loss: 1.7461 - val_acc: 0.3967\n",
      "Epoch 564/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.7365 - acc: 0.4006 - val_loss: 1.7463 - val_acc: 0.3967\n",
      "Epoch 565/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.7364 - acc: 0.4008 - val_loss: 1.7460 - val_acc: 0.3967\n",
      "Epoch 566/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.7366 - acc: 0.4007 - val_loss: 1.7459 - val_acc: 0.3968\n",
      "Epoch 567/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.7365 - acc: 0.4006 - val_loss: 1.7458 - val_acc: 0.3967\n",
      "Epoch 568/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.7359 - acc: 0.4010 - val_loss: 1.7458 - val_acc: 0.3968\n",
      "Epoch 569/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.7364 - acc: 0.4007 - val_loss: 1.7458 - val_acc: 0.3969\n",
      "Epoch 570/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.7365 - acc: 0.4007 - val_loss: 1.7455 - val_acc: 0.3969\n",
      "Epoch 571/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.7360 - acc: 0.4009 - val_loss: 1.7457 - val_acc: 0.3969\n",
      "Epoch 572/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.7359 - acc: 0.4009 - val_loss: 1.7458 - val_acc: 0.3967\n",
      "Epoch 573/1000\n",
      "1562/1562 [==============================] - 38s 24ms/step - loss: 1.7360 - acc: 0.4010 - val_loss: 1.7457 - val_acc: 0.3970\n",
      "Epoch 574/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562/1562 [==============================] - 37s 23ms/step - loss: 1.7359 - acc: 0.4010 - val_loss: 1.7456 - val_acc: 0.3969\n",
      "Epoch 575/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7356 - acc: 0.4010 - val_loss: 1.7454 - val_acc: 0.3971\n",
      "Epoch 576/1000\n",
      "1562/1562 [==============================] - 37s 23ms/step - loss: 1.7359 - acc: 0.4011 - val_loss: 1.7452 - val_acc: 0.3972\n",
      "Epoch 577/1000\n",
      "1562/1562 [==============================] - 36s 23ms/step - loss: 1.7359 - acc: 0.4010 - val_loss: 1.7452 - val_acc: 0.3971\n",
      "Epoch 578/1000\n",
      "1562/1562 [==============================] - 37s 23ms/step - loss: 1.7354 - acc: 0.4011 - val_loss: 1.7452 - val_acc: 0.3971\n",
      "Epoch 579/1000\n",
      "1562/1562 [==============================] - 37s 23ms/step - loss: 1.7354 - acc: 0.4013 - val_loss: 1.7449 - val_acc: 0.3970\n",
      "Epoch 580/1000\n",
      "1562/1562 [==============================] - 37s 23ms/step - loss: 1.7357 - acc: 0.4010 - val_loss: 1.7450 - val_acc: 0.3973\n",
      "Epoch 581/1000\n",
      "1562/1562 [==============================] - 37s 23ms/step - loss: 1.7352 - acc: 0.4013 - val_loss: 1.7449 - val_acc: 0.3972\n",
      "Epoch 582/1000\n",
      "1562/1562 [==============================] - 37s 23ms/step - loss: 1.7354 - acc: 0.4013 - val_loss: 1.7449 - val_acc: 0.3971\n",
      "Epoch 583/1000\n",
      "1562/1562 [==============================] - 37s 23ms/step - loss: 1.7354 - acc: 0.4012 - val_loss: 1.7450 - val_acc: 0.3971\n",
      "Epoch 584/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7352 - acc: 0.4014 - val_loss: 1.7446 - val_acc: 0.3975\n",
      "Epoch 585/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7352 - acc: 0.4012 - val_loss: 1.7446 - val_acc: 0.3972\n",
      "Epoch 586/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7353 - acc: 0.4013 - val_loss: 1.7447 - val_acc: 0.3973\n",
      "Epoch 587/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7345 - acc: 0.4017 - val_loss: 1.7446 - val_acc: 0.3974\n",
      "Epoch 588/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7351 - acc: 0.4014 - val_loss: 1.7444 - val_acc: 0.3975\n",
      "Epoch 589/1000\n",
      "1562/1562 [==============================] - 37s 24ms/step - loss: 1.7349 - acc: 0.4014 - val_loss: 1.7446 - val_acc: 0.3974\n",
      "Epoch 590/1000\n",
      "1271/1562 [=======================>......] - ETA: 6s - loss: 1.7350 - acc: 0.4015"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Reshape, Dropout, BatchNormalization, Activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(81,10)))\n",
    "model.add(Dense(810))\n",
    "model.add(Activation('relu'))\n",
    "# model.add(Dense(64))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dropout(0.3))\n",
    "model.add(Dense(810))\n",
    "model.add(Reshape((81,10)))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rsmprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='./logs/{}'.format(int(time.time())), histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "# history = model.fit(x_train, y_train, validation_split=0.125, epochs=epochs, batch_size=batch, callbacks=[tbCallBack])\n",
    "history = model.fit_generator(\n",
    "    generator=SudokuSequence(x_train, y_train, batch), \n",
    "    steps_per_epoch=len(x_train)//batch, \n",
    "    epochs=epochs, callbacks=[tbCallBack], \n",
    "    validation_data=SudokuSequence(x_test, y_test, batch), \n",
    "    validation_steps=len(x_test)//batch)\n",
    "\n",
    "# score = model.evaluate(x_test, y_test, batch_size=batch)\n",
    "# print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model(\"model_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsrAwxhhTJAsLY4wxRfr/TpHxtR8gErAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x235f1ba3048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xl8XHW9//HXJ1uTJmnTvWnaNN3o\n3qZtgAKVyipUFPCqyBUEBCr+XEBRQe79XVDk/sCLCIqKBYqoiHDZVGTHslmBpqV0S0v3Nm3apGna\npEvW+fz+mFMZQ9Jmm0ySeT8fj3l05nu+Z+ZzGB7zzjnfc87X3B0REZG2Soh1ASIi0r0pSEREpF0U\nJCIi0i4KEhERaRcFiYiItIuCRERE2kVBIhIlZpZnZm5mSS3oe7mZvdXe9xGJBQWJCGBmW8ys1swG\nNmpfHvyI58WmMpGuT0Ei8qHNwMVHXpjZVCAtduWIdA8KEpEP/Q74UsTry4DfRnYws75m9lszKzOz\nrWb2n2aWECxLNLM7zWyPmW0CPtnEug+aWYmZ7TCzH5lZYmuLNLNhZvZnM9trZhvM7OqIZSeYWaGZ\nVZrZbjO7K2hPNbPfm1m5me0zsyVmNqS1ny3SFAWJyIfeBvqY2cTgB/4i4PeN+vwc6AuMBuYSDp4r\ngmVXA+cBM4AC4LON1n0YqAfGBn3OBq5qQ52PAsXAsOAz/tvMzgiW3QPc4+59gDHA40H7ZUHdI4AB\nwDXA4TZ8tshHKEhE/tWRvZKzgLXAjiMLIsLl++5e5e5bgJ8AlwZdPg/c7e7b3X0v8P8i1h0CnAtc\n5+4H3b0U+CnwhdYUZ2YjgDnADe5e7e7LgQciaqgDxprZQHc/4O5vR7QPAMa6e4O7L3X3ytZ8tkhz\nFCQi/+p3wL8Dl9PosBYwEEgBtka0bQVygufDgO2Nlh0xEkgGSoJDS/uAXwODW1nfMGCvu1c1U8OV\nwHHA2uDw1XkR2/Ui8Ecz22lmPzaz5FZ+tkiTFCQiEdx9K+FB93nAU40W7yH8l/3IiLZcPtxrKSF8\n6Chy2RHbgRpgoLtnBY8+7j65lSXuBPqbWWZTNbj7ene/mHBA3QE8YWbp7l7n7j9w90nAyYQPwX0J\nkQ6gIBH5qCuB0939YGSjuzcQHnO4zcwyzWwk8G0+HEd5HPimmQ03s37AjRHrlgAvAT8xsz5mlmBm\nY8xsbmsKc/ftwGLg/wUD6NOCeh8BMLNLzGyQu4eAfcFqDWZ2mplNDQ7PVRIOxIbWfLZIcxQkIo24\n+0Z3L2xm8TeAg8Am4C3gD8DCYNn9hA8fvQ8s46N7NF8ifGhsDVABPAFkt6HEi4E8wnsnTwM3u/vL\nwbJzgNVmdoDwwPsX3L0aGBp8XiVQBLzOR08kEGkT08RWIiLSHtojERGRdlGQiIhIuyhIRESkXRQk\nIiLSLnFxW+qBAwd6Xl5erMsQEelWli5dusfdBx2rX1wESV5eHoWFzZ3NKSIiTTGzrcfupUNbIiLS\nTgoSERFpFwWJiIi0S1yMkTSlrq6O4uJiqqurY11Kp0hNTWX48OEkJ+uGryLSseI2SIqLi8nMzCQv\nLw8zi3U5UeXulJeXU1xczKhRo2Jdjoj0MHF7aKu6upoBAwb0+BABMDMGDBgQN3tfItK54jZIgLgI\nkSPiaVtFpHPFdZAcy4HqOkqr9Fe8iMjRKEiOoqqmnt37a6itD3X4e5eXl5Ofn09+fj5Dhw4lJyfn\nn69ra2tb9B5XXHEF69at6/DaRERaI2pBYmYjzGyRmRWZ2Wozu7aJPueb2QozW25mhWY2J2LZZWa2\nPnhcFtE+y8xWmtkGM/uZRfGYzYD0FMApP1jT8e89YADLly9n+fLlXHPNNXzrW9/65+uUlBQgPEge\nCjUfYg899BDjx4/v8NpERFojmnsk9cD17j4RmA18zcwmNerzKjDd3fOBLwMPAJhZf+Bm4ETgBODm\nYOpSgF8B84FxweOcaG1ASlIiWamJ7D1YS0OocyYA27BhA1OmTOGaa65h5syZlJSUMH/+fAoKCpg8\neTI//OEP/9l3zpw5LF++nPr6erKysrjxxhuZPn06J510EqWlpZ1Sr4hI1E7/DeaoLgmeV5lZEZBD\neJrRI30ORKySDhz5tf4E8LK77wUws5eBc8zsNaCPu/8jaP8tcAHwfHtq/cFfVrNmZ+VHFzTUQEM9\nB70XvZISSEpsee5OGtaHmz81uU31rFmzhoceeoj77rsPgNtvv53+/ftTX1/Paaedxmc/+1kmTfrX\nTN6/fz9z587l9ttv59vf/jYLFy7kxhtvbOrtRUQ6VKeMkZhZHjADeKeJZRea2Vrgr4T3SiAcONsj\nuhUHbTnB88btTX3m/OBwWWFZWVkbC08EnGQLUdfQeVMSjxkzhuOPP/6frx999FFmzpzJzJkzKSoq\nYs2aNR9ZJy0tjXPPPReAWbNmsWXLls4qV0TiXNQvSDSzDOBJ4Dp3/8if/e7+NPC0mZ0K3AqcCTQ1\n7uFHaf9oo/sCYAFAQUHBUVOg2T0HD8Hu1dQmprG2ZiCjBqaTmRr9K8PT09P/+Xz9+vXcc889vPvu\nu2RlZXHJJZc0eT3IkXEVgMTEROrr66Nep4gIRHmPxMySCYfII+7+1NH6uvsbwBgzG0h4T2NExOLh\nwM6gfXgT7dFhCdC7P8l1VaQlhCir6vhB92OprKwkMzOTPn36UFJSwosvvtjpNYiIHE00z9oy4EGg\nyN3vaqbP2CNnXZnZTCAFKAdeBM42s37BIPvZwIvBuEuVmc0O1vsS8KdobQMAvQdiQHbKIQ7U1FPX\n0PGnAh/NzJkzmTRpElOmTOHqq6/mlFNO6dTPFxE5FnOPzrH/4FTeN4GVwJFf35uAXAB3v8/MbiAc\nBnXAYeC77v5WsP6Xg/4At7n7Q0F7AfAbII3wIPs3/BgbUVBQ4I0ntioqKmLixIkt25jyDYTqqlld\nP5ycfr3pn55y7HW6oFZts4jEPTNb6u4Fx+oXzbO23qLpMY3IPncAdzSzbCGwsIn2QmBKR9TYYr0H\nklCxmayEw1RVJ3fbIBERiQZd2d4SqX0gIZmBCVUcqK4nFKW9OBGR7khB0hLBoHtq6CB4A4dqG2Jd\nkYhIl6EgaamUDAxIs1qqqutiXY2ISJehIGmp5N4AZCXWUlWtazRERI5QkLRUYhIk9iI9oZbqugZq\n63V4S0QEFCStk9KblNBhgHbvlXTEbeQBFi5cyK5du9pVi4hIe8TtnO1tkpKOHa4gPTFEVXU9AzJ6\ntfmtjtxGHuCWW24hIyOD73znO61+n4ULFzJz5kyGDh3a5lpERNpDQdIawThJv+Q6dtYkEnInIQrT\noTz88MP84he/oLa2lpNPPpl7772XUCjEFVdcwfLly3F35s+fz5AhQ1i+fDkXXXQRaWlpvPvuu/9y\nzy0Rkc6gIAF4/kbYtbIFHR1qD5KVkERKQxKekgjNBcnQqXDu7a0uZdWqVTz99NMsXryYpKQk5s+f\nzx//+EfGjBnDnj17WLkyXOe+ffvIysri5z//Offeey/5+fmt/iwRkY6gIGkVA0vAPHzHF3dvPkja\n6JVXXmHJkiUUFITvSnD48GFGjBjBJz7xCdatW8e1117LvHnzOPvsszv0c0VE2kpBAq3bc9hfDAfL\n2RwayaDMVIb2Te3QUtydL3/5y9x6660fWbZixQqef/55fvazn/Hkk0+yYMGCDv1sEZG20FlbrZXc\nGyNEZmI9NVE4BfjMM8/k8ccfZ8+ePUD47K5t27ZRVlaGu/O5z32OH/zgByxbtgyAzMxMqqqqOrwO\nEZGW0h5Ja6WEJ53KTKyhvK7tZ201Z+rUqdx8882ceeaZhEIhkpOTue+++0hMTOTKK6/E3TEz7rgj\nfK/LK664gquuukqD7SISM1G7jXxX0u7byEdyh92rOJSQzsbaAUzO6ROVM7eiQbeRF5HWaOlt5HVo\nq7XMILk3vULVOE5tfedOdCUi0tVEc4bEEWa2yMyKzGy1mV3bRJ8vmtmK4LHYzKYH7ePNbHnEo9LM\nrguW3WJmOyKWzYvWNjQrJZ3EUA0JhKip061SRCS+RXOMpB643t2XmVkmsNTMXnb3NRF9NgNz3b3C\nzM4FFgAnuvs6IB/AzBKBHcDTEev91N3vbG+BR8YbWi0pPDbSi3qq60P0bW8hnSAeDmGKSGxEbY/E\n3UvcfVnwvAooAnIa9Vns7hXBy7eB4U281RnARnff2pH1paamUl5e3rYf2MRwkKQlNlBT1/UPbbk7\n5eXlpKZ27KnKIiLQSWdtmVkeMAN45yjdriQ8B3tjXwAebdT2dTP7ElBIeK+novFKZjYfmA+Qm5v7\nkTcdPnw4xcXFlJWVtWALGvEQ7C/lYMJhDpLGwdKu/wOdmprK8OFN5bSISPtE/awtM8sAXgduc/en\nmulzGvBLYI67l0e0pwA7gcnuvjtoGwLsARy4Fch29y8frYamztpqtztGsSxzLl/YeRFFPzyHxITu\nceaWiEhLdYmztswsGXgSeOQoITINeAA4PzJEAucCy46ECIC773b3BncPAfcDJ0Sn+mPol8ew0C5q\n60Ns33soJiWIiHQF0Txry4AHgSJ3v6uZPrnAU8Cl7v5BE10uptFhLTPLjnh5IbCqYypupf6j6Fez\nE4D1pQdiUoKISFcQzT2SU4BLgdMjT9U1s2vM7Jqgz38BA4BfBsv/efzJzHoDZxEOmkg/NrOVZrYC\nOA34VhS3oXn98kg5uINEGlhfqluUiEj8itpgu7u/BRx14MDdrwKuambZIcIh07j90g4psL36jcJC\n9UzLPMCG3dojEZH4pSvb26pfHgAnZu1nQ5mCRETil4KkrYIgmZy2lw2lBwiFdMGfiMQnBUlb9RkG\niSmMSSzjUG0DO/cfjnVFIiIxoSBpq4REyMplcMMuALbvVZCISHxSkLRHv1FkHt4OwI59ChIRiU8K\nkvbol0dK5VbMnOIKXZQoIvFJQdIe/UdhNVWMy6inuEJ7JCISnxQk7RGcuTUjs0J7JCIStxQk7dFv\nFACTUvdqj0RE4paCpD36jQRgVGIZu/ZXU9/Q9ecmERHpaAqS9khJh4wh5Phu6kPO7qqaWFckItLp\nFCTt1S+PAXU7ACjW7eRFJA4pSNqrXx4Zh4oBNE4iInFJQdJe/UaRWLWTFOoUJCISlxQk7dUvD8OZ\nllnJjn06tCUi8UdB0l7BmVtTe+/XHomIxKVoTrU7wswWmVmRma02s2ub6PNFM1sRPBab2fSIZVuC\nmRAbz5zY38xeNrP1wb/9orUNLdJ3BADjUysUJCISl6K5R1IPXO/uE4HZwNfMbFKjPpuBue4+DbgV\nWNBo+Wnunu/uBRFtNwKvuvs44NXgdexkZkNCEiMTy9m57zANmpdEROJM1ILE3UvcfVnwvAooAnIa\n9Vns7hXBy7eB4S146/OBh4PnDwMXdEzFbZSYBH1yyPbS8LUkldUxLUdEpLN1yhiJmeUBM4B3jtLt\nSuD5iNcOvGRmS81sfkT7EHcvgXBYAYOb+cz5ZlZoZoVlZWXtKf/YsnLpVxeel0S3kxeReBP1IDGz\nDOBJ4Dp3r2ymz2mEg+SGiOZT3H0mcC7hw2KntuZz3X2Buxe4e8GgQYPaWH0LZY0k/fBOAN28UUTi\nTlSDxMySCYfII+7+VDN9pgEPAOe7e/mRdnffGfxbCjwNnBAs2m1m2cG62UBp9LaghbJySTq4K3wt\niWZKFJE4E82ztgx4EChy97ua6ZMLPAVc6u4fRLSnm1nmkefA2cCqYPGfgcuC55cBf4rOFrRCVi4A\nUzIqdeaWiMSdpCi+9ynApcBKM1setN0E5AK4+33AfwEDgF+Gc4f64AytIcDTQVsS8Ad3fyF4j9uB\nx83sSmAb8LkobkPLZIVPAZ6aUclGjZGISJyJWpC4+1uAHaPPVcBVTbRvAqZ/dA0IDn+d0RE1dphg\nj2R8rwpe1xiJiMQZXdneETKHgSWSl1TOjn2HCelaEhGJIwqSjpCYBH1zyA6VUtfglGpeEhGJIwqS\njpI1kv714WtJtmleEhGJIwqSjpKVS/qh8LUkW/YcjHExIiKdR0HSUfqOIPHgLnonNrBJQSIicURB\n0lGycjGcWf0OsXnPgVhXIyLSaRQkHSU4BTg/s5LN2iMRkTiiIOkoQZBMSK1gS/khnQIsInFDQdJR\n+uSEryVJ3ENtfYid+3WFu4jEBwVJRwnmJRkSCt9DUoe3RCReKEg6UlYufWrC15IoSEQkXihIOlLW\nCJKriklPSWRTmYJEROKDgqQjZeViVTsZNzBFeyQiEjcUJB0pKxc8RH7fwwoSEYkbCpKO1H80ANNT\nSymuOERNfUOMCxIRib5ozpA4wswWmVmRma02s2ub6PNFM1sRPBab2fRjrWtmt5jZDjNbHjzmRWsb\nWm3oVADG+yZCDtt180YRiQPR3COpB65394nAbOBrZjapUZ/NwFx3nwbcCixo4bo/dff84PFcFLeh\ndXplwoCx5BxeB6ABdxGJC1ELEncvcfdlwfMqoAjIadRnsbtXBC/fBoa3dN0uKzufzIrVgE4BFpH4\n0CljJGaWB8wA3jlKtyuB51u47teDw2ELzaxfM58538wKzaywrKysjZW3QfZ0EiqLGZNerSARkbgQ\n9SAxswzgSeA6d69sps9phIPkhhas+ytgDJAPlAA/aeo93X2Buxe4e8GgQYM6ZFtaJDs81fzH++zU\n7eRFJC5ENUjMLJlwEDzi7k8102ca8ABwvruXH2tdd9/t7g3uHgLuB06I5ja0WhAks1K2aY9EROJC\nNM/aMuBBoMjd72qmTy7wFHCpu3/QknXNLDvi5YXAqo6uvV3SsqBfHuNDmyirqqGqui7WFYmIRFVS\nFN/7FOBSYKWZLQ/abgJyAdz9PuC/gAHAL8PZQb27FzS3bnCG1o/NLB9wYAvwlShuQ9tk55O9dSkQ\nHnCfNjwrxgWJiERP1ILE3d8C7Bh9rgKuas267n5phxQYTdnT6b3mGfpwgOXb9ylIRKRH05Xt0TAs\nH4Azs3azaG1pjIsREYkuBUk0ZIeD5NwBu1i8sZzDtbpVioj0XAqSaOjdH/rmMj1pKzX1Id7eVH7s\ndUREuikFSbRkT2Ng1VrSkhNZtE6Ht0Sk51KQRMuwfBL2buCM0Wn8bW0p7h7rikREokJBEi3BOMmF\ng0sorjjMxrIDMS5IRCQ6FCTRMvIUSOvHyXufAWDR2k6835eISCdSkERLSm84/mrSNr7AGQP3a5xE\nRHqsFgWJmY0xs17B84+b2TfNTFfZHcsJ8yGpF99Ie4F3N+/V7VJEpEdq6R7Jk0CDmY0lfA+sUcAf\nolZVT5ExCKZfzLTy58kK7eP5VbtiXZGISIdraZCE3L2e8E0S73b3bwHZx1hHAE7+Bhaq43v9X+P2\n59dSfqAm1hWJiHSolgZJnZldDFwGPBu0JUenpB5mwBhs4nn8W8MLNFRXcctf1sS6IhGRDtXSILkC\nOAm4zd03m9ko4PfRK6uHOeU6Emv282jOkzz7fjEvrdYhLhHpOVp09193XwN8EyCY2jbT3W+PZmE9\nyvAC+PhNTHrtv/lJ3wT+8+lUThw1gL69tVMnIt1fS8/aes3M+phZf+B94CEza3KyKmnG3O/BiV/l\nMzV/5uLqx/j3B95m577Dsa5KRKTdWnpoq28wZ/pngIfcfRZwZvTK6oHM4BP/DdMv5ltJ/8sXyn/J\n53/+Ksu2VcS6MhGRdmlpkCQFU9x+ng8H24/KzEaY2SIzKzKz1WZ2bRN9vmhmK4LHYjObHrHsHDNb\nZ2YbzOzGiPZRZvaOma03s8fMLKWF2xB7CQnw6Xvh+Ku51J7jsdC3ufv+B/j16xuprtOt5kWke2pp\nkPwQeBHY6O5LzGw0sP4Y69QD17v7RGA28DUzm9Soz2ZgrrtPA24FFgCYWSLwC+BcYBJwccS6dwA/\ndfdxQAVwZQu3oWtITIJP3gmX/5WhfXvz28QfMfqVq/n2j+/l8Xe3Ud8QinWFIiKtYp11V1oz+xNw\nr7u/3MzyfsAqd88xs5OAW9z9E8Gy7wfdbgfKgKHuXt+4X3MKCgq8sLCww7alw9Qdhrfupu7tX5Nc\nU8Hq0EieSzmb9OkXMu+kfPIGpse6QhGJY2a21N0LjtWvRWdtmdlw4OfAKYADbwHXuntxC9fPA2YA\n7xyl25XA88HzHGB7xLJi4ERgALAvuDjySHtOM585H5gPkJub25IyO19yGpz2fZLnXIeveJzcN+7l\nu/vvJ1T4AO8umcCrmXNInXAWM2fNZkJ2H8yanMZeRCSmWhQkwEOEb4nyueD1JUHbWcda0cwyCN9i\n5bpgwL6pPqcRDpI5R5qa6OZHaf9oo/sCgkNlBQUFXXsykOQ0bNZlZM66DErXcnDZE4xb8RSzD94P\nS++npLA/LyROoWrQLDLHncy4KScwenBfEhIULCISey0NkkHu/lDE69+Y2XXHWsnMkgmHyCPu/lQz\nfaYBDwDnuvuROWmLgRER3YYDO4E9QJaZJQV7JUfae47BE8g85z/hnP+EfdupXP0SdStf4OSyQvru\nfgN2/5RDb/ZiuY2iNGMioewZ9BldwOgJ+WRn9dZei4h0upYGyR4zuwR4NHh9MXDUicgt/Iv2IFDk\n7k1ec2JmucBTwKXu/kHEoiXAuOAK+h3AF4B/d3c3s0XAZ4E/Er5ly59auA3dT9YI+pxyJX1OuRLc\n8X1bKV3zJvvX/4P+pSuYfOCv9Fr/NKyHqhfSeM/yKE8fS+3AifQePo0hY2cweng2qcmJsd4SEenB\nWjTYHvzg30v4NikOLAa+6e7bjrLOHOBNYCVw5FSkm4BcAHe/z8weAP4N2Bosrz8ysGNm84C7gURg\nobvfFrSPJhwi/YH3gEvc/ah3Quyyg+3t1VBHdUkRu9b+g+qtS0ktX83gwxvp7R9e6LjdB1GcnMeB\njFHYwLGkD5vIkDH55A4fTlKipqMRkea1dLC9zWdtmdl17n53m1buZD02SJriTkPFVnavX0bV1vfx\n3avJqNzA4LpiUvhwPpS9nklxUi77MsbSMHAiacOnMGRsPiOG5ShgRATonCDZ5u5d9HSofxVXQdKc\nUAPVe7aya9MKKrevIlT6ARmV6xlas4UMDv2zW6lnsSN5JPszxxIaNJneI6YybGw+w4cM0uC+SJzp\njCDZ7u4jjt0z9hQkR+HO4fJt7Fq/jKrtq/DSIjIrN5Bdu4U0PjxiuNMHsCtlJJWZ4/ChU8nMm8Hw\nsdMZ0i9DA/wiPZT2SCIoSNog1MCBXesp3fAeB4pXY3s+IKNqI8PqttIrOERW64lstuHsThtDdb8J\npAybzIDRMxg9+jjSU3VnY5HurkMuSDSzKpq+TsOAtDbWJt1BQiIZwyaQMWzCv7Y31LO/eA2l6wup\nLl5BSnkRkw6uYODOv4VPxC6E/d6b9xNHUZF5HKEhU8jMm8GI8QUM6a+LKkV6ok67RUosaY8k+kIH\nKyjb9B57N71HXclq0iuKyK7ZRG+qAajzRLZYDqVpY6gZMJG0EdMZMuFkRo4YQaLGXkS6pKgf2upO\nFCQxEgpxYNcH7PpgCYe2LSe5dBUDD21kUKjsn12KfRDbUidwcOA0UvMKyJl0MnnZQzSwL9IFKEgi\nKEi6lrqDFexc+y77N7xDYsl7DKxczZDQbgBCbmxjKCVpY6keOJXUUScyYsoccgYP0GExkU6mIImg\nIOn66itLKVn7Nvs2vkvCrhUMqFrH0FB4bvt6T+ADy6MkYzIN2bPof9yJjJs8i769e8W2aJEeTkES\nQUHSPdVWlbNz1RtUrf87absLyT64lnTCV+1Xehrrk8azt38+yaNOYsS0uYwaNlSHxEQ6kIIkgoKk\nhwg1cGBnETtWvUXtlnfoW/4+OXWbSMRpcGOd5bEjczqh4ScyaPJcJo2foPuMibSDgiSCgqTn8upK\ndq15i71Fr5O6812GHVz9zwspi30gm1Mnc3joLPqOP5UJ006ib0ZqjCsW6T4UJBEUJHGkoY7Kzcso\nWfUaoW3vMHjfcgaEwjeq3u+9WZM8lcohx5M5fi4TZsyhf2bvGBcs0nUpSCIoSOJb9Z6tbH/vFWo2\nvsHAPe8ytD48hc1B78Xa5EmUDzmF9IlnMXnGbLLStccicoSCJIKCRCLVVuyg+P1XOfDBGwwofYec\n+vBsCHu8D2tTpnAwezb9Jp/B5Okn6lYvEtcUJBEUJHI0tXu3U7z0eWrWv8bA8iUMaigFYJf3Y03v\n46nOPY2cmWcxaewYknWLfYkjMQ8SMxsB/BYYSnhiqwXufk+jPhMIz/0+E/gPd78zaB8PPBbRdTTw\nX+5+t5ndAlwNHLk8+iZ3f+5otShIpDWqy7awrfCvNKx/lREVb5PhBwFY5yPZ2reAhLGnM+74TzAy\ne1CMKxWJrq4QJNlAtrsvM7NMYClwgbuviegzGBgJXABUHAmSRu+TSHi63RPdfWsQJAea6tscBYm0\nWUM9VZvfZed7L5K49U1GHFhBL+qo8SRWJU6kbMgc+kw5h6kzTyYzLSXW1Yp0qA65+297uHsJUBI8\nrzKzIiAHWBPRpxQoNbNPHuWtzgA2uvvWo/QRiY7EJDLHnsz4sScD4LWH2LXqNcpXvMCQHW8wq+RX\nUPIrdr+UxZLeJ1A3+izyjp/HcSNzdEsXiRtRC5JIZpYHzADeacPqXwAebdT2dTP7ElAIXO/uFU18\n5nxgPkBubreYNkW6AUvpzdCZ8xg6cx4AtXuL2bbkWerWvcQJFX8nY/VL1K26kfcSJrB7yKn0nf5J\n8mfOpncvDdpLzxX1wXYzywBeB25z96ea6XMLTRyuMrMUwrNcTHb33UHbEGAP4XlSbiV8+OzLR6tB\nh7akUzTUs/eDv7O78E9kbH+NEbUbAdjhAynKmI2PO5vxJ32K3CH9Y1yoSMvE/NBWUEQy8CTwSHMh\ncgznAsuOhAhA5HMzux94tt2FinSExCT6T5xL/4lzgWBv5Z0/Ub/uBU7Z9wppy5/lwHvf4/WUAvbn\nnUPe7AuYMmq47g8m3V7UgsTCB4gfBIrc/a42vs3FNDqsZWbZwfgLwIXAqrZXKRI9Kf2HM/bcr8G5\nX4P6GnaveIW9S59ieskrZK1/i7oPbmZpwkTKhp5K/xmfJn/GCbo3mHRL0Txraw7wJrCS8Om/ADcB\nuQDufp+ZDSU8ztEn6HMAmOShYfKXAAAUC0lEQVTulWbWG9gOjHb3/RHv+zsgn/ChrS3AVyKCpUk6\ntCVdSqiBqg1/p+TdZ0jftoic2k0AbPZsNvY/lV5TPsX0k8+mT5puky+xFfPTf7sSBYl0ZTXlW9m6\n+ElY9zyjDiwjmXpKPYuVmR+DiZ9i+sfOY2Cf9FiXKXFIQRJBQSLdRcOhfWx9+xlqVj5DXsVi0qhh\nr2ewPH0OoYmfZtrHPs3grMxYlylxQkESQUEi3ZHXHqK48FkOLHuC3D1vkM5h9noGy3p/jNDkC5nx\nsfMY1Fd7KhI9CpIIChLp9uqq2bH0L1QVPs7IPW+QRjXlnsny9Dn4pAuYceqnGKDDX9LBFCQRFCTS\no9QeYufSv1C5NLyn0ptqyr0P72XOxab+GwVz5tE3XQP10n4KkggKEumpvPYQO5b8hQPLHiev/A1S\nqWWX92dl1mn0nnkRs046g9SUTrmBhfRACpIIChKJB15Txfa3n+LQsicYvX8xKdSz1YfyweBz6H/S\nJeTnF5Coix+lFRQkERQkEm8aDlWw8Y3HYOXjjD2wjARzVtpxbMu9gLxTL2XS6BG6qaQck4IkgoJE\n4ln13mI2LXqYvmv/l5y6zdR4MkuSZ3F4/AVMP/0iBg/Qvb+kaQqSCAoSEcCdqs1L2fH6Qwze/hz9\nQ3s56L14L+NUyP8iM0/9JL17aU4V+ZCCJIKCRKSRUAM7V7xK2eLfM7b0JdI5zHYfzNrB8xhy6hVM\nnTJdh75EQRJJQSLSvFDNQTa9+Uf8vd8z5sB7JJizInES5WM/y6SzL2fIgAGxLlFiREESQUEi0jKH\nyray4dWFDFj/BDkNxVR5GkszTyfl+MspOPkMUnR34riiIImgIBFpJXd2rHyNvW/cz7g9L5NKLZsY\nztbhn2b0mVcxMm9MrCuUTqAgiaAgEWm7hkP7+GDR70he8UfG1qyi3hNYmnYSdfmXM+u0C0nTNMI9\nloIkgoJEpGOUb13NtlfuY9T2Z8iiku0MYW32hYw882qOGzM21uVJB1OQRFCQiHQsr6tm/euPYst+\nw7hDy6nzRJb0mk39zMs54fQLSU3RXkpPEPMgMbMRwG+BoYRnP1zg7vc06jMBeAiYCfyHu98ZsWwL\nUAU0APVHNsbM+gOPAXmEZ0j8vLtXHK0WBYlI9OzfvoZtL/+S3O3P0Ner2M4Q1udcyOizvkJe3uhY\nlyft0BWCJBvIdvdlZpYJLAUucPc1EX0GAyOBC4CKJoKkwN33NHrfHwN73f12M7sR6OfuNxytFgWJ\nSPR53WHWv/4oLP0Nxx1+nzpPZFnabHzm5cw87TOkJOvmkd1NS4MkIVoFuHuJuy8LnlcBRUBOoz6l\n7r4EqGvFW58PPBw8f5hwCIlIjFlyGsed+WWOu+ENyq/4O6tG/Dvjq1cye/HV7LxtKq88/CN2lu45\n9htJt9MpYyRmlge8AUxx98omlt8CHGi0R7IZqAAc+LW7Lwja97l7VkS/Cnfv18R7zgfmA+Tm5s7a\nunVrR26SiLRAqLaatYt+R9rS+xlVu44qT2NJ1jn0n/tVps84QVfPd3ExP7QVUUgG8Dpwm7s/1Uyf\nW/hokAxz953B4a+XgW+4+xstDZJIOrQlEmPulBa9Sdmr9zKu/FVSqOf9xKnsn3oZs86+hPTeabGu\nUJoQ80NbQRHJwJPAI82FSHPcfWfwbynwNHBCsGh3MP5yZBymtOMqFpGoMGPwpFOZ/I3HCV23mhXj\nr2Oo7+bU5d/hwB0TWbTgO2zfrqMG3VXUgsTC+6wPAkXuflcr100PBugxs3TgbGBVsPjPwGXB88uA\nP3VMxSLSGVKzhjLt4h8w+D+K2HDGAvZmjOW0nfcz+IFZvPmTi1m57G3i4bKEniSaZ23NAd4EVhI+\n/RfgJiAXwN3vM7OhQCHQJ+hzAJgEDCS8FwKQBPzB3W8L3ncA8HjwPtuAz7n73qPVokNbIl3bni0r\nKX7+J0zY/Syp1LE8aRoHpl3BrLO/SFqq5p+PlS4zRtIVKEhEuofD+0pZ99y9ZK//A0O8jF0MYO3w\nzzP+vG+SPXRYrMuLOwqSCAoSke7FG+pY/+aThN75NRMOL+Owp1CYdQ6DzryWCVOP+bsmHURBEkFB\nItJ97Vq/lF0v/ZSJZS/QizqW9jqBhJO/xvQ5nyYhMarnC8U9BUkEBYlI93dwbwlr/3I3ozf/gX5U\nsj5hNGXTrmHmuZeT2kvjKNGgIImgIBHpOepqDrHq+fsZuGIBI0LFFDOYDaO/xLRPfY3+/frHurwe\nRUESQUEi0vN4qIF1rz9G0ts/Z2zNGiq9NysGn0/evG8xfNT4WJfXIyhIIihIRHq27e+/RvnffsaU\nfYsAeL/Px+l31ncYPe2UGFfWvSlIIihIROLDnh0b2fiXO5lc8jQZdpjVqTOwj32biSedhyVoYL61\nFCQRFCQi8WX/vnJW/elujtv8OwZRwQdJx3Hg+GvJP/NiEhITY11et6EgiaAgEYlP1YcPsvzZ+xix\n5tfk+G62JoygdNo15M+7muQUnel1LAqSCAoSkfhWX1fL+y8+TL9l9zI6tIVdDGT7xKuY9umv0yst\nM9bldVkKkggKEhEB8FCI9xc9Tq+372Fi3Rr20ofNY7/EpPO/Q1rmUWejiEsKkggKEhGJ5O6sXPwC\nDW/cyYyaQipJZ93ILzLhwu+RmTUo1uV1GQqSCAoSEWnOqsLXqX71DgoO/52DpLJ2+Oc57sIbyRyQ\nc+yVe7guMbGViEhXN6VgLgU3PMe6C19kVfpJ5G//Hck/n87yX8+nsnRbrMvrFhQkIiLA+OmzOfG7\nz7DxotcozDyDyTufoNcvZrL0119h3+7tsS6vS4vmDIkjzGyRmRWZ2Wozu7aJPhPM7B9mVmNm32nJ\numZ2i5ntMLPlwWNetLZBROLPcZPymXP9Y2y++HUK+5zJ9J2Pk/rLGRT++hr2lypQmhLNGRKzgWx3\nXxZMm7sUuMDd10T0GQyMBC4AKtz9zmOta2a3AAeO9G0JjZGISFtt/mAFu5/9Ecfvf4laklmT81nG\nfeb/0mdgz59oK+ZjJO5e4u7LgudVQBGQ06hPqbsvAepau66ISGcYddw0Zn/7cbZc/DrL+5xG/o5H\nSb43n6UPfpMDFaWxLq9L6JQxEjPLA2YA73TQul83sxVmttDMmjz528zmm1mhmRWWlZW1umYRkUhj\nJkznpOsfZ+PnF/F++inM2PZbuGcayx76NtX74/s3JupBYmYZwJPAde5e2QHr/goYA+QDJcBPmlrX\n3Re4e4G7FwwapPPCRaRjHDd5BrO/+zQf/NtLrEkrIH/LQhp+OoVVD3+L2sr4DJSoBomZJRMOgkfc\n/amOWNfdd7t7g7uHgPuBEzqyZhGRlpgw7QROuOFZVp3/PMtSZzNp00PU3jWNlY/+J/WHW/U3c7cX\nzbO2DHgQKHL3uzpq3WAg/ogLgVXtrVVEpK2mzTyJOTc8w5JP/pWVydOZuu7nVP54Ciue+h9CtdWx\nLq9TRPOsrTnAm8BKIBQ03wTkArj7fWY2FCgE+gR9DgCTgGlNrevuz5nZ7wgf1nJgC/AVdy85Wi06\na0tEOoO78+6bL5D2+q1Ma1jN7oTB7Dvheo476yosMSnW5bWabpESQUEiIp2poSHEP17+Xwa+cwcT\nfCPFSbnUfPy/GHPKZ8Es1uW1WMxP/xURiVeJiQnMOeciRt30Ln+b9j801Ncx5pWr+OD2j7F1+d9i\nXV6HU5CIiERJr+QkTv/MfAZ+7z0Wjb2R/jXbGPnMhay98yzKiv4e6/I6jIJERCTK0nuncdol3yfp\nuvd5dcTXGVJVxKDH5vHB3edRtX11rMtrNwWJiEgnycrqxxlX3sah/7OM5wZdRXZFIWkPzmH1giup\nrjjqOUNdmoJERKST5QwZzLyv/YRdly/m9cxPcdyOp2m4J581j95EqBteg6IgERGJkXGjRnPG9b9l\n9YUvsjxlFpPW/YL9P57Kpufuhoa6Y79BF6EgERGJsfz84znpxr/y+sceZQvDGP3uzZTcPpOSJc9A\nN7hEQ0EiItIFJCQYc8+Yx8Tvv8lfJ99FdW0d2X+9jI13ncX+rStiXd5RKUhERLqQ1JQkPvm5K8n4\nViF/GXYdAyrXkP7QXFbe/xWqK8tjXV6TFCQiIl3QoKwMPjX/B+z58tu8kflJJhU/Rs1d01j7xA/x\nmgOxLu9fKEhERLqwsSNzOf3637PivL+wLmk8E1b9hP13TGH3y3dDfW2sywMUJCIi3cKM4z/GzO+/\nwgsn/Ib1oWEM+fvN7PnxDPa993TMB+QVJCIi3URSYgLnzLuQcd9dxCNjf8K+GifrT5dTfPeZ1OxY\nGbO6FCQiIt1MVnovvnjJVSR89e/8YeA3Sd+3lqT7T2Xzb/8Pfmhvp9ejIBER6aZGD+3Hv3/9VjZc\n9AbP9TqX3I1/oOp/prPzpZ9BQ32n1aEgERHp5o6fNIZ5NzzCy6f+L+s8l2GL/y+l/zOLg6ue75TP\nj+ZUuyPMbJGZFZnZajO7tok+E8zsH2ZWY2bfabTsHDNbZ2YbzOzGiPZRZvaOma03s8fMLCVa2yAi\n0l0kJhjnnHEWx31vEY+Ovp1Dhw6T/sQXWPfqw1H/7GjukdQD17v7RGA28DUzm9Soz17gm8CdkY1m\nlgj8AjiX8NS7F0esewfwU3cfB1QAV0ZvE0REupe+vVO4+Etf5dDVf2dhv2/SN/+CqH9m1ILE3Uvc\nfVnwvAooAnIa9Sl19yVA47uTnQBscPdN7l4L/BE438wMOB14Iuj3MBD9/0oiIt3MpBGD+PK1tzJ0\nQN+of1anjJGYWR4wA3inhavkANsjXhcHbQOAfe5e36i9qc+cb2aFZlZYVlbWlrJFRKQFoh4kZpYB\nPAlc5+4tvdG+NdHmR2n/aKP7AncvcPeCQYMGtfBjRUSktaIaJGaWTDhEHnH3p1qxajEwIuL1cGAn\nsAfIMrOkRu0iIhIj0Txry4AHgSJ3v6uVqy8BxgVnaKUAXwD+7O4OLAI+G/S7DPhTR9UsIiKtl3Ts\nLm12CnApsNLMlgdtNwG5AO5+n5kNBQqBPkDIzK4DJrl7pZl9HXgRSAQWuvvq4D1uAP5oZj8C3iMc\nViIiEiNRCxJ3f4umxzQi++wifHiqqWXPAc810b6J8FldIiLSBejKdhERaRcFiYiItIt5N5hYvr3M\nrAzY2sbVBxI+WyzexON2x+M2Q3xudzxuM7R+u0e6+zGvn4iLIGkPMyt094JY19HZ4nG743GbIT63\nOx63GaK33Tq0JSIi7aIgERGRdlGQHNuCWBcQI/G43fG4zRCf2x2P2wxR2m6NkYiISLtoj0RERNpF\nQSIiIu2iIDmK5qb77UmamxLZzPqb2cvBlMYvm1m/WNfa0cws0czeM7Nng9c9fhpnM8sysyfMbG3w\nnZ/U079rM/tW8P/2KjN71MxSe+J3bWYLzazUzFZFtDX53VrYz4LfthVmNrM9n60gacYxpvvtSZqb\nEvlG4NVgSuNXg9c9zbWEZ+48Ih6mcb4HeMHdJwDTCW9/j/2uzSyH8HTeBe4+hfBNYL9Az/yufwOc\n06itue/2XGBc8JgP/Ko9H6wgaV6T0/3GuKYOd5Qpkc8nPJUx9MApjc1sOPBJ4IHgdY+fxtnM+gCn\nEtwx291r3X0fPfy7Jnxz2rRgHqPeQAk98Lt29zeAvY2am/tuzwd+62FvE57nKbutn60gaV5z0/32\nWI2mRB7i7iUQDhtgcOwqi4q7ge8BoeB1i6dx7sZGA2XAQ8EhvQfMLJ0e/F27+w7gTmAb4QDZDyyl\n53/XRzT33Xbo75uCpHktnta3J2jjlMjdkpmdB5S6+9LI5ia69rTvOwmYCfzK3WcAB+lBh7GaEowJ\nnA+MAoYB6YQP6zTW077rY+nQ/98VJM1rbrrfHqeZKZF3H9nVDf4tjVV9UXAK8Gkz20L4kOXphPdQ\nevo0zsVAsbu/E7x+gnCw9OTv+kxgs7uXuXsd8BRwMj3/uz6iue+2Q3/fFCTNa3K63xjX1OGOMiXy\nnwlPZQw9bEpjd/++uw939zzC3+vf3P2L9PBpnIOJ5Lab2fig6QxgDT34uyZ8SGu2mfUO/l8/ss09\n+ruO0Nx3+2fgS8HZW7OB/UcOgbWFrmw/CjObR/gv1SPT/d4W45I6nJnNAd4EVvLheMFNhMdJHic8\nNfI24HPu3nggr9szs48D33H388xsNOE9lP6Ep3G+xN1rYllfRzOzfMInGKQAm4ArCP9B2WO/azP7\nAXAR4TMU3wOuIjwe0KO+azN7FPg44VvF7wZuBp6hie82CNV7CZ/ldQi4wt0L2/zZChIREWkPHdoS\nEZF2UZCIiEi7KEhERKRdFCQiItIuChIREWkXBYlIBzCzBjNbHvHosCvGzSwv8o6uIl1N0rG7iEgL\nHHb3/FgXIRIL2iMRiSIz22Jmd5jZu8FjbNA+0sxeDeaCeNXMcoP2IWb2tJm9HzxODt4q0czuD+bV\neMnM0mK2USKNKEhEOkZao0NbF0Usq3T3EwhfSXx30HYv4dt4TwMeAX4WtP8MeN3dpxO+D9bqoH0c\n8At3nwzsA/4tytsj0mK6sl2kA5jZAXfPaKJ9C3C6u28Kbo65y90HmNkeINvd64L2EncfaGZlwPDI\n23UEt/d/OZicCDO7AUh29x9Ff8tEjk17JCLR5808b65PUyLvA9WAxjelC1GQiETfRRH//iN4vpjw\nnYcBvgi8FTx/Ffgq/HNO+T6dVaRIW+mvGpGOkWZmyyNev+DuR04B7mVm7xD+w+3ioO2bwEIz+y7h\nWQuvCNqvBRaY2ZWE9zy+SnhmP5EuS2MkIlEUjJEUuPueWNciEi06tCUiIu2iPRIREWkX7ZGIiEi7\nKEhERKRdFCQiItIuChIREWkXBYmIiLTL/wc+HwPyIWtJDwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x235f1b927b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quiz\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4  5  6  7  8\n",
       "0  7  0  1  0  2  8  3  9  0\n",
       "1  2  3  0  0  0  4  8  0  0\n",
       "2  0  0  9  0  0  0  0  0  4\n",
       "3  9  0  0  0  3  2  1  7  0\n",
       "4  0  0  8  0  9  0  5  4  0\n",
       "5  1  0  6  0  0  7  0  0  8\n",
       "6  4  6  0  0  0  9  7  0  0\n",
       "7  0  1  0  0  6  0  0  0  0\n",
       "8  0  0  7  5  0  0  0  0  3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4  5  6  7  8\n",
       "0  7  4  1  6  2  8  3  9  5\n",
       "1  2  3  5  9  1  4  8  6  7\n",
       "2  6  8  9  3  7  5  2  1  4\n",
       "3  9  5  4  8  3  2  1  7  6\n",
       "4  3  7  8  1  9  6  5  4  2\n",
       "5  1  2  6  4  5  7  9  3  8\n",
       "6  4  6  3  2  8  9  7  5  1\n",
       "7  5  1  2  7  6  3  4  8  9\n",
       "8  8  9  7  5  4  1  6  2  3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4  5  6  7  8\n",
       "0  7  4  1  9  8  8  3  9  3\n",
       "1  7  3  9  1  7  4  8  8  2\n",
       "2  8  8  9  3  2  3  4  7  4\n",
       "3  9  9  9  4  3  2  7  8  8\n",
       "4  1  7  8  9  9  9  1  4  6\n",
       "5  1  2  3  7  1  7  9  9  9\n",
       "6  3  3  8  7  2  9  8  1  5\n",
       "7  5  1  7  2  4  3  9  1  4\n",
       "8  6  9  7  7  8  1  9  1  3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.argmax(preprocessing_data(dt[\"quizzes\"][x_test[0]]), axis=1)\n",
    "y = np.argmax(preprocessing_data(dt[\"solutions\"][y_test[0]]), axis=1)\n",
    "\n",
    "p = model.predict(preprocessing_data(dt[\"quizzes\"][x_test[0]]).reshape((1,81,10)))\n",
    "p = p.reshape((81,10))\n",
    "p = [np.argmax(i) for i in p]\n",
    "\n",
    "puz = create_dataframe(x)\n",
    "sol = create_dataframe(y)\n",
    "pre = create_dataframe(p)\n",
    "\n",
    "print(\"quiz\")\n",
    "display(puz)\n",
    "print(\"solution\")\n",
    "display(sol)\n",
    "print(\"predicted\")\n",
    "display(pre)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sudoku",
   "language": "python",
   "name": "sudoku"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
