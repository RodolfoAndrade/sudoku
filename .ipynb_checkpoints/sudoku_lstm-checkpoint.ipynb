{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third approach to sudoku solver using Long-Short Term Memory networks\n",
    "As done previously we will skip the explanation of some of these familiar blocks of code. \n",
    "\n",
    "In this notebook, we explain the lstm approach and the results achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "# import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the new x dataset\n",
    "On this note, we load the saved new dataset on the previous notebook sudoku_algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 10, 81)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data_x = np.load(\"new_data_x.npy\")\n",
    "new_data_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 10, 81, 10)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "new_data_x = to_categorical(new_data_x, num_classes=10)\n",
    "new_data_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the old y dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 2)\n"
     ]
    }
   ],
   "source": [
    "dt = pd.read_csv(\"sudoku_dataset/sudoku.csv\")\n",
    "print(dt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "def preprocessing_data(data):\n",
    "    array = [list(map(int,list(i))) for i in data]\n",
    "    return to_categorical(array, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_size = 4900\n",
    "test_size = 100\n",
    "epochs = 10\n",
    "batch = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 10, 810)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data_x = np.reshape(new_data_x, (new_data_x.shape[0], new_data_x.shape[1], -1))\n",
    "new_data_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and test data\n",
    "In this code, we use the variable y as the solution obtained from the original dataset, while the x was from the modified version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4900, 10, 810)\n"
     ]
    }
   ],
   "source": [
    "x_train = new_data_x[:train_size]\n",
    "y_train = preprocessing_data(dt[\"solutions\"][:train_size])\n",
    "\n",
    "x_test = new_data_x[train_size:train_size+test_size]\n",
    "y_test = preprocessing_data(dt[\"solutions\"][train_size:train_size+test_size])\n",
    "\n",
    "print(x_train.shape)\n",
    "# print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing\n",
    "Here, different from the previous models we use an LSTM layer imported from keras. LSTM networks are one type of Recurrent Neural Network (RNN). RNNs are capable of finding information from a sequence of data. The LSTM networks are better in the search for information on a long period of time.\n",
    "\n",
    "The first parameter in the LSTM layer is the output. The input is the number of steps and features. Then comes the fully connected layer with 810 units. After that, the reshape layer and the softmax activation function.\n",
    "\n",
    "With this configuration, we reach 99% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_14 (LSTM)               (None, 810)               5252040   \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 810)               656910    \n",
      "_________________________________________________________________\n",
      "reshape_13 (Reshape)         (None, 81, 10)            0         \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 81, 10)            0         \n",
      "=================================================================\n",
      "Total params: 5,908,950\n",
      "Trainable params: 5,908,950\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4287 samples, validate on 613 samples\n",
      "Epoch 1/10\n",
      "4287/4287 [==============================] - 452s 105ms/step - loss: 0.0561 - acc: 0.6117 - val_loss: 0.0270 - val_acc: 0.9041\n",
      "Epoch 2/10\n",
      "4287/4287 [==============================] - 423s 99ms/step - loss: 0.0130 - acc: 0.9728 - val_loss: 0.0123 - val_acc: 0.9745\n",
      "Epoch 3/10\n",
      "4287/4287 [==============================] - 440s 103ms/step - loss: 0.0040 - acc: 0.9977 - val_loss: 0.0074 - val_acc: 0.9873\n",
      "Epoch 4/10\n",
      "4287/4287 [==============================] - 611s 142ms/step - loss: 0.0014 - acc: 0.9996 - val_loss: 0.0054 - val_acc: 0.9916\n",
      "Epoch 5/10\n",
      "4287/4287 [==============================] - 468s 109ms/step - loss: 6.6143e-04 - acc: 0.9998 - val_loss: 0.0045 - val_acc: 0.9932\n",
      "Epoch 6/10\n",
      "4287/4287 [==============================] - 569s 133ms/step - loss: 4.0249e-04 - acc: 0.9998 - val_loss: 0.0041 - val_acc: 0.9932\n",
      "Epoch 7/10\n",
      "4287/4287 [==============================] - 502s 117ms/step - loss: 2.9441e-04 - acc: 0.9998 - val_loss: 0.0039 - val_acc: 0.9928\n",
      "Epoch 8/10\n",
      "4287/4287 [==============================] - 450s 105ms/step - loss: 2.3457e-04 - acc: 0.9998 - val_loss: 0.0037 - val_acc: 0.9927\n",
      "Epoch 9/10\n",
      "4287/4287 [==============================] - 529s 123ms/step - loss: 1.9410e-04 - acc: 0.9998 - val_loss: 0.0035 - val_acc: 0.9927\n",
      "Epoch 10/10\n",
      "4287/4287 [==============================] - 511s 119ms/step - loss: 1.6451e-04 - acc: 0.9998 - val_loss: 0.0034 - val_acc: 0.9929\n",
      "100/100 [==============================] - 3s 27ms/step\n",
      "[0.0034411631990224124, 0.9909876561164856]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Reshape, Dropout, BatchNormalization, Activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(810, input_shape=x_train.shape[1:]))\n",
    "model.add(Dense(810))\n",
    "model.add(Reshape((81,10)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='./logs/{}'.format(int(time.time())), histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_split=0.125, epochs=epochs, batch_size=batch, callbacks=[tbCallBack])\n",
    "\n",
    "score = model.evaluate(x_test, y_test, batch_size=batch)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "We test the results with the first case of test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x_train_1 = np.reshape(x_test[0], (1,10,810))\n",
    "y_test_1 = model.predict(x_train_1)\n",
    "y_test_1 = np.argmax(y_test_1[0], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print all digits that are different from the solution. None digits was printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for i,j in zip(y_test_1,np.argmax(y_test[0], axis=1)):\n",
    "    if i!=j:\n",
    "        print(i,j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Conclusion\n",
    "This approach was perfect to solve the sudoku puzzle. We had to create a new dataset to teach the network how to solve the quizzes. But it took just 5000 games and 10 epochs to the neural network to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sudoku",
   "language": "python",
   "name": "sudoku"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
